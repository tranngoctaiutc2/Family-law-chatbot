import asyncio
import os
import re
import json
import time
import logging
from datetime import datetime
from textwrap import dedent
from typing import List, Dict, Any, Optional, Tuple

import gradio as gr
from dotenv import load_dotenv
from qdrant_client import QdrantClient
from sentence_transformers import SentenceTransformer
import google.generativeai as genai
from qdrant_client.http.models import Filter, FieldCondition, MatchValue
from tenacity import retry, stop_after_attempt, wait_exponential
from rank_bm25 import BM25Okapi

# ================== Cáº¤U HÃŒNH MÃ”I TRÆ¯á»œNG ==================
load_dotenv()
QDRANT_URL = os.getenv("QDRANT_URL", "").strip()
QDRANT_API_KEY = os.getenv("QDRANT_API_KEY", "").strip()
COLLECTION_NAME = os.getenv("COLLECTION_NAME", "")
EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL", "BAAI/bge-m3")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY", "").strip()
GEMINI_MODEL_ID = os.getenv("GEMINI_MODEL_ID", "models/gemini-1.5-flash")

INTENT_DEBUG = os.getenv("INTENT_DEBUG", "0").strip() in {"1", "true", "TRUE", "yes", "on"}
LOG_LEVEL = os.getenv("LOG_LEVEL", "INFO").upper()
CASUAL_MAX_WORDS = int(os.getenv("CASUAL_MAX_WORDS", "0").strip() or 0)
INTENT_RAW_PREVIEW_LIMIT = int(os.getenv("INTENT_RAW_PREVIEW_LIMIT", "240").strip() or 240)
INTENT_FALLBACK_CASUAL = os.getenv(
    "INTENT_FALLBACK_CASUAL",
    "ChÃ o báº¡n, mÃ¬nh cÃ³ thá»ƒ há»— trá»£ cÃ¢u há»i vá» Luáº­t HÃ´n nhÃ¢n & Gia Ä‘Ã¬nh. Báº¡n muá»‘n há»i ná»™i dung gÃ¬?",
).strip()

if not (QDRANT_URL and QDRANT_API_KEY):
    raise RuntimeError("Thiáº¿u QDRANT_URL hoáº·c QDRANT_API_KEY trong tá»‡p .env")

# ================== THIáº¾T Láº¬P LOGGING ==================
class KVFormatter(logging.Formatter):
    def format(self, record: logging.LogRecord) -> str:
        base = super().format(record)
        extras = []
        extras_dict = getattr(record, "__kv__", {})
        if isinstance(extras_dict, dict):
            for k, v in extras_dict.items():
                try:
                    extras.append(f"{k}={v}")
                except Exception:
                    continue
        return base + (" | " + ",".join(extras) if extras else "")

LOG_FORMAT = "%(asctime)s | %(levelname)s | %(name)s | %(message)s"
handlers = [
    logging.StreamHandler(),
    logging.FileHandler("botchat_honnhan.log", encoding="utf-8"),
]
for h in handlers:
    h.setFormatter(KVFormatter(LOG_FORMAT))

root_logger = logging.getLogger()
root_logger.handlers = []
root_logger.setLevel(getattr(logging, LOG_LEVEL, logging.INFO))
for h in handlers:
    root_logger.addHandler(h)

app_log = logging.getLogger("app")
metrics_logger = logging.getLogger("metrics")
if not metrics_logger.handlers:
    fh = logging.FileHandler("metrics.log", encoding="utf-8")
    fh.setFormatter(logging.Formatter("%(message)s"))
    metrics_logger.addHandler(fh)
    metrics_logger.setLevel(logging.INFO)

def log_step(event: str, **kv):
    kvpairs = ",".join([f"{k}={v}" for k, v in kv.items()])
    metrics_logger.info(f"ts={int(time.time())},evt={event},{kvpairs}")

def log_time(func):
    import functools
    @functools.wraps(func)
    def sync_wrapper(*args, **kwargs):
        t0 = time.perf_counter()
        try:
            return func(*args, **kwargs)
        finally:
            elapsed = time.perf_counter() - t0
            app_log.info(
                f"Thá»i gian thá»±c thi {func.__name__}",
                extra={"__kv__": {"thoi_gian": f"{elapsed:.4f} giÃ¢y"}},
            )
    return sync_wrapper

# ================== KHá»I Táº O ==================
client = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY, prefer_grpc=True)
embedder = SentenceTransformer(EMBEDDING_MODEL)

genai.configure()
INTENT_SYSTEM_PROMPT = dedent("""
Báº¡n lÃ  trá»£ lÃ½ vá» Luáº­t HÃ´n nhÃ¢n & Gia Ä‘Ã¬nh Viá»‡t Nam.
Tráº£ vá» **JSON thuáº§n** (khÃ´ng markdown, khÃ´ng lá»i dáº«n).

Schema má»™t trong cÃ¡c dáº¡ng:
1) {"intent":"casual","answer":"..."}
2) {"intent":"legal_answer","normalized_query":"...","original_query":"..."}
3) {"intent":"law_search","filters":{"article_no":int?,"clause_no":int?,"point_letter":str?,"chapter_number":int?}}

Quy táº¯c xÃ¡c Ä‘á»‹nh intent:
- Há»i vá» Ä‘iá»u/khoáº£n/chÆ°Æ¡ng/má»¥c cá»¥ thá»ƒ â†’ law_search.
- Há»i xÃ£ giao/chÃ o há»i â†’ casual.
- Nháº¯c sá»‘ Ä‘iá»u/khoáº£n nhÆ°ng há»i tÃ¬nh huá»‘ng thá»±c táº¿, Ã¡p dá»¥ng, thá»§ tá»¥c â†’ legal_answer.
- LuÃ´n dá»±a vÃ o **má»¥c Ä‘Ã­ch cÃ¢u há»i**, khÃ´ng chá»‰ dá»±a vÃ o sá»‘ Ä‘iá»u/khoáº£n.

Náº¿u intent = casual thÃ¬ báº¯t buá»™c cÃ³ answer (tiáº¿ng Viá»‡t, lá»‹ch sá»±).
""")
gemini_model = genai.GenerativeModel(
    model_name=GEMINI_MODEL_ID,
    system_instruction=INTENT_SYSTEM_PROMPT,
)
answer_model = genai.GenerativeModel(model_name=GEMINI_MODEL_ID)

# ================== HÃ€M Há»– TRá»¢ ==================
def _safe_truncate(text: str, limit: int = 800) -> str:
    return text if text and len(text) <= limit else (text[:limit] + "â€¦(cáº¯t)") if text else ""

LEGAL_HINTS = re.compile(
    r"(?i)\b(Ä‘iá»u|khoáº£n|Ä‘iá»ƒm|chÆ°Æ¡ng|hÃ´n nhÃ¢n|ly hÃ´n|ly thÃ¢n|nuÃ´i con|tÃ i sáº£n|"
    r"quan há»‡ vá»£ chá»“ng|káº¿t hÃ´n|há»§y káº¿t hÃ´n|chung sá»‘ng nhÆ° vá»£ chá»“ng|cáº¥p dÆ°á»¡ng|giÃ¡m há»™)\b"
)

def looks_like_legal(query: str) -> bool:
    return bool(LEGAL_HINTS.search(query or ""))

class SimpleTTLCache:
    def __init__(self, ttl_seconds: int = 1800, max_items: int = 512):
        self.ttl = ttl_seconds
        self.max = max_items
        self.store: Dict[str, Tuple[float, Any]] = {}

    def _evict_if_needed(self):
        if len(self.store) <= self.max:
            return
        oldest_key = min(self.store, key=lambda k: self.store[k][0])
        self.store.pop(oldest_key, None)

    def get(self, key: str):
        item = self.store.get(key)
        if not item:
            return None
        ts, value = item
        if time.time() - ts > self.ttl:
            self.store.pop(key, None)
            return None
        return value

    def set(self, key: str, value: Any):
        self.store[key] = (time.time(), value)
        self._evict_if_needed()

embed_cache = SimpleTTLCache(ttl_seconds=3600, max_items=1024)
search_cache = SimpleTTLCache(ttl_seconds=900, max_items=1024)

@log_time
def encode_query(text: str):
    key = f"{EMBEDDING_MODEL}|query|{text}"
    v = embed_cache.get(key)
    if v is not None:
        return v
    vec = embedder.encode([f"query: {text}"], normalize_embeddings=True)[0].tolist()
    embed_cache.set(key, vec)
    return vec

# ================== PHÃ‚N TÃCH Ã Äá»ŠNH (INTENT) ==================
@log_time
def _intent_via_gemini(query: str) -> Dict[str, Any]:
    try:
        cfg = genai.types.GenerationConfig(
            temperature=0.0,
            max_output_tokens=192,
            response_mime_type="application/json",
        )
        resp = gemini_model.generate_content(
            [
                {
                    "role": "user",
                    "parts": [f"CÃ¢u há»i: {query}\nHÃ£y tráº£ JSON thuáº§n phÃ¹ há»£p schema Ä‘Ã£ nÃªu."],
                }
            ],
            generation_config=cfg,
        )

        candidates = getattr(resp, "candidates", None) or []
        first_cand = candidates[0] if candidates else None
        finish_reason = getattr(first_cand, "finish_reason", None)
        safety = []
        try:
            if first_cand and getattr(first_cand, "safety_ratings", None):
                for s in first_cand.safety_ratings:
                    cat = getattr(s, "category", "")
                    prob = getattr(s, "probability", "")
                    safety.append(f"{cat}:{prob}")
        except Exception:
            pass

        raw = getattr(resp, "text", "") or ""
        app_log.info(
            "Káº¿t quáº£ phÃ¢n tÃ­ch Ã½ Ä‘á»‹nh",
            extra={
                "__kv__": {
                    "do_dai": len(raw),
                    "xem_truoc": _safe_truncate(raw, INTENT_RAW_PREVIEW_LIMIT),
                    "so_ung_vien": len(candidates),
                    "ly_do_ket_thuc": finish_reason,
                    "bao_mat": ";".join(safety[:6]),
                }
            },
        )

        if finish_reason == 2 or not raw:
            log_step("intent_block", ly_do=str(finish_reason))
            app_log.warning(
                "PhÃ¢n tÃ­ch Ã½ Ä‘á»‹nh bá»‹ cháº·n",
                extra={"__kv__": {"ly_do_ket_thuc": finish_reason, "do_dai_raw": len(raw)}}
            )
            return {"intent": "casual", "answer": INTENT_FALLBACK_CASUAL}

        data = json.loads(raw) if raw else {}
        if not isinstance(data, dict):
            app_log.warning("Káº¿t quáº£ phÃ¢n tÃ­ch khÃ´ng pháº£i dict")
            return {"intent": "casual", "answer": INTENT_FALLBACK_CASUAL}
        out: Dict[str, Any] = {}
        for k in ("intent", "answer", "normalized_query", "filters", "original_query"):
            if k in data and data[k] not in (None, ""):
                out[k] = data[k]
        app_log.info(
            "Ã Ä‘á»‹nh Ä‘Ã£ phÃ¢n tÃ­ch",
            extra={
                "__kv__": {
                    "loai_y_dinh": out.get("intent", ""),
                    "co_tra_loi": int("answer" in out and bool(out.get("answer"))),
                    "co_bo_loc": int("filters" in out and bool(out.get("filters"))),
                    "do_dai_tra_loi": len(out.get("answer", "") or ""),
                    "ly_do_ket_thuc": finish_reason,
                }
            },
        )
        return out
    except Exception as e:
        app_log.warning("Lá»—i phÃ¢n tÃ­ch Ã½ Ä‘á»‹nh", extra={"__kv__": {"loi": str(e)}})
        return {"intent": "casual", "answer": INTENT_FALLBACK_CASUAL}

@log_time
def analyze_intent(query: str) -> Dict[str, Any]:
    data = _intent_via_gemini(query)
    intent = data.get("intent")
    answer = data.get("answer", "")
    normalized_query = data.get("normalized_query", "") or query
    original_query = data.get("original_query", "")
    filters = data.get("filters", {}) or {}

    if intent not in {"casual", "legal_answer", "law_search"}:
        if re.search(r"(?i)\bÄ‘iá»u\s*\d+", query) or re.search(r"(?i)\bkhoáº£n\s*\d+", query):
            intent = "law_search"
        elif looks_like_legal(query):
            intent = "legal_answer"
        else:
            intent = "casual"
        log_step("intent_fallback", do_dai_query=len(query))

    log_step("intent", loai=intent, co_legal=str(looks_like_legal(query)))
    app_log.info("Quyáº¿t Ä‘á»‹nh Ã½ Ä‘á»‹nh", extra={"__kv__": {"loai_y_dinh": intent}})
    return {
        "intent": intent,
        "answer": answer,
        "normalized_query": normalized_query,
        "original_query": original_query,
        "filters": filters,
    }

# ================== TÃŒM KIáº¾M BM25 ==================
def tokenize(text):
    return re.findall(r'\w+', text.lower())

@log_time
def rank_by_bm25(docs: list, query: str):
    corpus = [tokenize(d['content']) for d in docs]
    bm25 = BM25Okapi(corpus)
    tokenized_query = tokenize(query)
    scores = bm25.get_scores(tokenized_query)
    for d, s in zip(docs, scores):
        d['bm25_score'] = float(s)
    ranked_docs = sorted(docs, key=lambda x: x['bm25_score'], reverse=True)
    return ranked_docs

@log_time
def rerank_bm25(query: str, docs: list) -> list:
    corpus = [d.get("content", "") for d in docs]
    if not corpus:
        return docs

    bm25 = BM25Okapi([doc.split() for doc in corpus])
    scores = bm25.get_scores(query.split())

    for d, s in zip(docs, scores):
        d["bm25_score"] = float(s)

    reranked = sorted(docs, key=lambda x: x["bm25_score"], reverse=True)

    print("DEBUG: Káº¿t quáº£ sáº¯p xáº¿p láº¡i BM25:")
    for i, d in enumerate(reranked, 1):
        print(
            f"  {i}. Äiá»ƒm BM25={d['bm25_score']:.4f}, "
            f"Äiá»ƒm gá»‘c={d.get('score', 0.0):.4f}, "
            f"Ná»™i dung xem trÆ°á»›c={d['content'][:50]}..."
        )

    return reranked

# ================== TÃŒM KIáº¾M HYBRID ==================
@log_time
def _build_filter(query_text: str) -> Optional[Filter]:
    conds: List[FieldCondition] = []
    m = re.search(r"(?i)\bÄ‘iá»u\s*(\d+)\b", query_text)
    if m:
        conds.append(FieldCondition(key="article_no", match=MatchValue(value=int(m.group(1)))))
    m = re.search(r"(?i)\bkhoáº£n\s*(\d+)\b", query_text)
    if m:
        conds.append(FieldCondition(key="clause_no", match=MatchValue(value=int(m.group(1)))))
    m = re.search(r"(?i)\bÄ‘iá»ƒm\s*([a-z])\b", query_text)
    if m:
        conds.append(FieldCondition(key="point_letter", match=MatchValue(value=m.group(1).lower())))
    m = re.search(r"(?i)\bchÆ°Æ¡ng\s*(\d+)\b", query_text)
    if m:
        conds.append(FieldCondition(key="chapter_number", match=MatchValue(value=int(m.group(1)))))
    return Filter(must=conds) if conds else None

@log_time
def search_law(query: str, top_k: int = 15, score_threshold: float = 0.42):
    t0 = time.perf_counter()
    app_log.info(
        "Báº¯t Ä‘áº§u tÃ¬m kiáº¿m",
        extra={"__kv__": {"cau_hoi": _safe_truncate(query, 80), "top_k": top_k, "nguong_diem": score_threshold}},
    )

    cache_key = f"search|{COLLECTION_NAME}|{top_k}|{score_threshold}|{query}"
    cached = search_cache.get(cache_key)
    if cached is not None:
        app_log.info("TÃ¬m kiáº¿m tá»« bá»™ nhá»› cache")
        return cached

    try:
        t_embed0 = time.perf_counter()
        vec = encode_query(query)
        t_embed = time.perf_counter() - t_embed0

        flt = _build_filter(query)

        t_q0 = time.perf_counter()
        results = client.query_points(
            collection_name=COLLECTION_NAME,
            query=vec,
            with_payload=True,
            limit=max(1, min(int(top_k) * 3, 80)),
            query_filter=flt,
        )
        t_qdrant = time.perf_counter() - t_q0

        raw_docs = []
        for r in results.points:
            p = r.payload or {}
            raw_docs.append({
                "citation": p.get("exact_citation", ""),
                "chapter_number": p.get("chapter_number", ""),
                "article_no": p.get("article_no", ""),
                "article_title": p.get("article_title", ""),
                "clause_no": p.get("clause_no", ""),
                "point_letter": p.get("point_letter", ""),
                "content": (p.get("content") or "").strip(),
                "score": float(r.score or 0.0),
            })

        t_filter0 = time.perf_counter()
        filtered = [d for d in raw_docs if d.get("score", 0.0) >= score_threshold] or raw_docs[:max(1, top_k)]
        filtered = rerank_bm25(query, filtered)
        t_filter = time.perf_counter() - t_filter0

        selected = filtered[:top_k]
        selected = rank_by_bm25(selected, query)[:top_k]

        search_cache.set(cache_key, selected)

        sk_top1 = selected[0]['score'] if selected else 0.0
        log_step(
            "tim_kiem",
            k_yeu_cau=top_k,
            k_tra_ve=len(selected),
            diem_top1=f"{sk_top1:.4f}",
            t_nhung=f"{t_embed:.4f}",
            t_qdrant=f"{t_qdrant:.4f}",
            t_loc=f"{t_filter:.4f}",
            t_tong=f"{time.perf_counter()-t0:.4f}",
        )
        app_log.info(
            "TÃ¬m kiáº¿m hoÃ n táº¥t",
            extra={"__kv__": {"so_luong": len(selected), "diem_top1": f"{sk_top1:.4f}"}},
        )
        return selected
    except Exception as e:
        app_log.error("Lá»—i tÃ¬m kiáº¿m", extra={"__kv__": {"loi": str(e)}})
        log_step("tim_kiem_loi", thong_bao=str(e))
        raise

# ================== CÃ”NG Cá»¤ RENDER ==================
def law_line(d: Dict[str, Any]) -> Tuple[str, str, str]:
    art = d.get("article_no")
    cls = d.get("clause_no")
    pt = d.get("point_letter")
    parts = []
    if pt:
        parts.append(f"Äiá»ƒm {pt}")
    if cls:
        parts.append(f"Khoáº£n {cls}")
    if art:
        parts.append(f"Äiá»u {art}")
    cited = " ".join(parts)
    chapter = f" (ChÆ°Æ¡ng {d.get('chapter_number')})" if d.get("chapter_number") else ""
    title = f" â€” {d.get('article_title')}" if d.get("article_title") else ""
    return cited, chapter, title

def docs_to_markdown(docs: List[Dict[str, Any]]):
    if not docs:
        return "âŒ KhÃ´ng tÃ¬m tháº¥y Ä‘iá»u luáº­t nÃ o."
    lines = []
    for i, d in enumerate(docs, 1):
        cited, chapter, title = law_line(d)
        content = (d.get("content") or "").strip()
        score = round(d.get("score", 0.0), 4)
        lines.append(
            f"**{i}. {cited}{chapter}{title}**  \n"
            f"{content}  \n"
            f"<sub>Äá»™ liÃªn quan: {score}</sub>\n"
        )
    return "\n".join(lines)

def paginate_docs(docs, page: int, page_size: int):
    total = len(docs)
    if total == 0:
        return [], 0, 0, 0
    page = max(1, int(page))
    page_size = max(1, int(page_size))
    start = (page - 1) * page_size
    end = start + page_size
    sliced = docs[start:end]
    total_pages = (total + page_size - 1) // page_size
    return sliced, total, total_pages, start

def docs_page_markdown(docs, page: int, page_size: int):
    sliced, total, total_pages, start = paginate_docs(docs, page, page_size)
    if total == 0:
        return "(ChÆ°a cÃ³ dá»¯ liá»‡u)", "Trang 0/0"
    body = docs_to_markdown(sliced)
    page_label = f"Trang {page}/{total_pages} â€” hiá»ƒn thá»‹ {start+1}â€“{min(start+len(sliced), total)} / {total}"
    return f"**{page_label}**\n\n{body}", page_label

# ================== XÃ‚Y Dá»°NG PROMPT ==================
@log_time
def build_prompt(query: str, docs: List[Dict[str, Any]], history_msgs=None):
    history_block = ""
    if history_msgs:
        lines = []
        for i, m in enumerate(history_msgs[-5:], 1):
            role = m.get("role", "")
            content = m.get("content", "")
            role_label = "NgÆ°á»i dÃ¹ng" if role == "user" else "Trá»£ lÃ½"
            lines.append(f"- {i}. {role_label}: {content}")
        history_block = "\nLá»‹ch sá»­ há»™i thoáº¡i gáº§n Ä‘Ã¢y:\n" + "\n".join(lines)

    docs_sorted = sorted(
        docs,
        key=lambda d: (
            int(d.get("article_no") or 9999),
            int(d.get("clause_no") or 9999),
            str(d.get("point_letter") or ""),
        ),
    )

    context_lines = []
    for idx, d in enumerate(docs_sorted, 1):
        art = d.get("article_no")
        cls = d.get("clause_no")
        pt = d.get("point_letter")
        parts = []
        if pt:
            parts.append(f"Äiá»ƒm {pt}")
        if cls:
            parts.append(f"Khoáº£n {cls}")
        if art:
            parts.append(f"Äiá»u {art}")
        cited = " ".join(parts)
        chapter = f" (ChÆ°Æ¡ng {d.get('chapter_number')})" if d.get("chapter_number") else ""
        title = f" â€” {d.get('article_title')}" if d.get("article_title") else ""
        content = (d.get("content") or "").strip()
        context_lines.append(f"{idx}) {cited}{chapter}{title}: {content}")

    context = "\n".join(context_lines) if context_lines else "âŒ KhÃ´ng cÃ³ Ä‘iá»u luáº­t nÃ o."

    prompt = dedent(f"""
    Báº¡n lÃ  luáº­t sÆ° tÆ° váº¥n Luáº­t HÃ´n nhÃ¢n & Gia Ä‘Ã¬nh, chá»‰ dÃ¹ng trÃ­ch Ä‘oáº¡n trong danh sÃ¡ch sau.
    Quy táº¯c:
    - CÃ¢u há»i ÄÃºng/Sai â†’ tráº£ lá»i **Káº¿t luáº­n: ÄÃºng/Sai** + lÃ½ do.
    - CÃ¢u há»i thÆ°á»ng â†’ tráº£ lá»i **1â€“3 cÃ¢u**, bÃ¡m sÃ¡t cÃ¢u há»i.
    - **TrÃ­ch dáº«n nguyÃªn vÄƒn** Ä‘iá»u luáº­t liÃªn quan (Äiá»ƒmâ€“Khoáº£nâ€“Äiá»u + ná»™i dung), theo thá»© tá»±.
    - Náº¿u thiáº¿u cÄƒn cá»© â†’ tráº£ lá»i: **KhÃ´ng Ä‘á»§ cÄƒn cá»©.**
    - CÃ¢u há»i ngoÃ i luáº­t â†’ tráº£ lá»i lá»‹ch sá»±, ngáº¯n gá»n, khÃ´ng viá»‡n dáº«n luáº­t.
    Äá»ŠNH Dáº NG TRáº¢ Lá»œI:
    - TrÃ­ch dáº«n: <liá»‡t kÃª toÃ n bá»™ Äiá»ƒmâ€“Khoáº£nâ€“Äiá»u + ná»™i dung>
    - Giáº£i thÃ­ch: <1â€“3 cÃ¢u, Ã¡p dá»¥ng tÃ¬nh huá»‘ng>
    - Káº¿t luáº­n: <káº¿t luáº­n ngáº¯n gá»n dá»±a vÃ o cÃ¢u há»i vÃ  giáº£i thÃ­ch>

    CÃ¢u há»i hiá»‡n táº¡i:
    \"\"\"{query}\"\"\"{history_block}

    Danh sÃ¡ch Ä‘iá»u luáº­t (top_k):
    {context}
    """).strip()

    return prompt

# ================== Xá»¬ LÃ TRáº¢ Lá»œI LLM ==================
@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))
def _gemini_stream(prompt, temperature: float):
    cfg = genai.types.GenerationConfig(temperature=float(temperature))
    return answer_model.generate_content(prompt, generation_config=cfg, stream=True)

@log_time
def stream_answer(prompt, temperature=0.2):
    t0 = time.perf_counter()
    t_first0 = time.perf_counter()
    first_token_emitted = False
    try:
        resp = _gemini_stream(prompt, temperature)
        for ch in resp:
            if getattr(ch, "text", None):
                if not first_token_emitted:
                    log_step("llm_first_token", thoi_gian_truoc=f"{time.perf_counter()-t_first0:.4f}")
                    first_token_emitted = True
                yield ch.text
    except Exception as e:
        app_log.error("Lá»—i gá»i mÃ´ hÃ¬nh LLM", extra={"__kv__": {"loi": str(e)}})
        yield f"\n\nLá»—i gá»i mÃ´ hÃ¬nh: {e}"
    finally:
        log_step("llm_tong", thoi_gian=f"{time.perf_counter()-t0:.4f}")

# ================== Láº¤Y Dá»® LIá»†U QDRANT ==================
@log_time
def _fetch(filters: Dict[str, Any], limit: int = 10):
    must = []
    mapping = {"article_no": int, "clause_no": int, "point_letter": str, "chapter_number": int}
    for k, caster in mapping.items():
        if k in filters and filters[k] not in (None, ""):
            try:
                val = caster(filters[k])
                must.append(FieldCondition(key=k, match=MatchValue(value=val)))
            except Exception as e:
                app_log.warning(
                    "Lá»—i Ã©p kiá»ƒu dá»¯ liá»‡u bá»™ lá»c",
                    extra={"__kv__": {"truong": k, "gia_tri": filters[k], "loi": str(e)}},
                )
                try:
                    val = str(filters[k])
                    must.append(FieldCondition(key=k, match=MatchValue(value=val)))
                except:
                    pass
    app_log.info(
        "Bá»™ lá»c tÃ¬m kiáº¿m",
        extra={"__kv__": {"bo_loc_goc": str(filters), "dieu_kien_must": str(must)}},
    )
    if not must:
        app_log.warning("KhÃ´ng cÃ³ Ä‘iá»u kiá»‡n must")
        return []
    flt = Filter(must=must)
    out = []
    try:
        scroll_res, _ = client.scroll(
            collection_name=COLLECTION_NAME,
            scroll_filter=flt,
            limit=min(64, max(5, limit)),
            with_payload=True,
        )
        app_log.info(
            "Káº¿t quáº£ tÃ¬m kiáº¿m Qdrant",
            extra={"__kv__": {"so_luong": len(scroll_res), "collection": COLLECTION_NAME}},
        )
        for r in scroll_res:
            p = r.payload or {}
            out.append({
                "chapter": p.get("chapter", ""),
                "article_no": p.get("article_no", ""),
                "article_title": p.get("article_title", ""),
                "clause_no": p.get("clause_no", ""),
                "point_letter": p.get("point_letter", ""),
                "content": (p.get("content") or "").strip(),
                "score": 1.0,
            })
            if len(out) >= limit:
                break
    except Exception as e:
        app_log.error(
            "Lá»—i khi tÃ¬m kiáº¿m Qdrant",
            extra={"__kv__": {"loi": str(e), "collection": COLLECTION_NAME}},
        )
        return []
    if not out:
        app_log.warning("KhÃ´ng tÃ¬m tháº¥y káº¿t quáº£", extra={"__kv__": {"bo_loc": str(filters)}})
    return out

# ================== HÃ€M Há»– TRá»¢ GIAO DIá»†N ==================
def ui_return(msg_val, chatbot_val, cites_val, last_answer_val, docs_val, page_val, page_label_val, history_val):
    print("DEBUG: Gá»i hÃ m ui_return, tráº£ vá» 8 giÃ¡ trá»‹")
    return (
        msg_val,
        chatbot_val,
        gr.update(value=cites_val),
        last_answer_val,
        docs_val,
        page_val,
        page_label_val,
        history_val,
    )

# ================== GIAO DIá»†N NGÆ¯á»œI DÃ™NG ==================
CSS = """
#chatbot { height: 540px !important; }
label { font-size:12px !important; opacity:.9 }
#cites-box {
    max-height: 360px;
    overflow-y: auto;
    border: 1px solid #ddd;
    padding: 6px;
    border-radius: 6px;
    background-color: #fafafa;
}
#history_box {
    max-height: 200px;
}
"""

with gr.Blocks(
    title="âš–ï¸ Trá»£ lÃ½ Luáº­t HÃ´n NhÃ¢n & Gia ÄÃ¬nh 2014",
    css=CSS,
) as demo:
    gr.Markdown("""
    ### âš–ï¸ Trá»£ lÃ½ Luáº­t HÃ´n NhÃ¢n & Gia ÄÃ¬nh 2014
    *Tham chiáº¿u chÃ­nh xÃ¡c â€¢ Háº¡n cháº¿ suy diá»…n â€¢ KhÃ´ng thay tháº¿ tÆ° váº¥n phÃ¡p lÃ½*
    """)

    with gr.Row():
        with gr.Column(scale=7):
            chatbot = gr.Chatbot(
                value=[],
                type="messages",
                show_copy_button=True,
                elem_id="chatbot",
            )
            with gr.Row():
                ex1 = gr.Button("ChÃ o báº¡n")
                ex2 = gr.Button("Äiá»u 81 quy Ä‘á»‹nh gÃ¬ vá» viá»‡c nuÃ´i con sau ly hÃ´n")
                ex3 = gr.Button("Khoáº£n 2 Äiá»u 56 nÃ³i gÃ¬")
        with gr.Column(scale=5):
            history_box = gr.Chatbot(
                value=[],
                type="messages",
                show_copy_button=False,
                label="ğŸ“œ Lá»‹ch sá»­ chat",
                elem_id="history_box",
            )
            gr.Markdown("**CÆ¡ sá»Ÿ phÃ¡p lÃ½**")
            cites_md = gr.Markdown(value="(ChÆ°a cÃ³ dá»¯ liá»‡u)", elem_id="cites-box")
            with gr.Row():
                prev_page = gr.Button("â¬…ï¸")
                next_page = gr.Button("â¡ï¸")
            with gr.Row():
                page_info = gr.Markdown("Trang 0/0")
                page_size = gr.Slider(3, 20, value=5, step=1, label="Má»—i trang")

    with gr.Row():
        msg = gr.Textbox(placeholder="Nháº­p cÃ¢u há»i...", scale=5, autofocus=True)
        send = gr.Button("Gá»­i", variant="primary", scale=1)
        clear = gr.Button("LÃ m má»›i", scale=1)

    # Äiá»n sáºµn vÃ­ dá»¥
    def _fill(text):
        return text

    ex1.click(lambda: _fill("ChÃ o báº¡n"), outputs=msg)
    ex2.click(
        lambda: _fill("Äiá»u 81 quy Ä‘á»‹nh gÃ¬ vá» viá»‡c nuÃ´i con sau ly hÃ´n"),
        outputs=msg,
    )
    ex3.click(lambda: _fill("Khoáº£n 2 Äiá»u 56 nÃ³i gÃ¬"), outputs=msg)

    # Tráº¡ng thÃ¡i
    state_history = gr.State([])
    state_last_answer = gr.State("")
    state_docs = gr.State([])
    state_page = gr.State(1)

    # -------- Xá»­ lÃ½ chÃ­nh (Async Generator) --------
    @log_time
    def respond(message, history_msgs, cur_page_size, k=15, temperature=0.2, threshold=0.42):
        print(f"DEBUG: Báº¯t Ä‘áº§u xá»­ lÃ½ cÃ¢u há»i: {message}")
        if not (message and message.strip()):
            print("DEBUG: CÃ¢u há»i rá»—ng, tráº£ vá» máº·c Ä‘á»‹nh")
            gr.Info("Vui lÃ²ng nháº­p cÃ¢u há»i.")
            return ui_return(
                gr.update(),
                history_msgs,
                "",
                "",
                [],
                1,
                "Trang 0/0",
                history_msgs,
            )

        t_overall0 = time.perf_counter()
        try:
            # PhÃ¢n tÃ­ch Ã½ Ä‘á»‹nh
            print("DEBUG: Gá»i hÃ m phÃ¢n tÃ­ch Ã½ Ä‘á»‹nh")
            intent_info = analyze_intent(message)
            print(f"DEBUG: Káº¿t quáº£ Ã½ Ä‘á»‹nh: {intent_info}")
            intent = intent_info["intent"]
            intent_answer = intent_info.get("answer", "")
            normalized_query = intent_info.get("normalized_query", message)
            original_query = intent_info.get("original_query", message)
            intent_filters = intent_info.get("filters", {})

            # Xá»­ lÃ½ cÃ¢u há»i xÃ£ giao
            if intent == "casual":
                final_answer = (intent_answer or "").replace("\u200b", "").strip()
                app_log.info(
                    "Xá»­ lÃ½ cÃ¢u há»i xÃ£ giao",
                    extra={"__kv__": {"do_dai_tra_loi": len(final_answer)}},
                )

                if final_answer and CASUAL_MAX_WORDS > 0:
                    words = final_answer.split()
                    if len(words) > CASUAL_MAX_WORDS:
                        truncated = " ".join(words[:CASUAL_MAX_WORDS])
                        app_log.info(
                            "Cáº¯t ngáº¯n cÃ¢u tráº£ lá»i xÃ£ giao",
                            extra={
                                "__kv__": {
                                    "so_tu_goc": len(words),
                                    "so_tu_giu": CASUAL_MAX_WORDS,
                                    "do_dai_goc": len(final_answer),
                                }
                            },
                        )
                        final_answer = truncated

                if len(final_answer) >= 1:
                    history_msgs = history_msgs + [
                        {"role": "user", "content": message},
                        {"role": "assistant", "content": final_answer},
                    ]
                    print("DEBUG: Tráº£ vá» cÃ¢u tráº£ lá»i xÃ£ giao trá»±c tiáº¿p")
                    return ui_return(
                        gr.update(value=""),
                        history_msgs,
                        "(KhÃ´ng cÃ³ trÃ­ch dáº«n)",
                        final_answer,
                        [],
                        1,
                        "Trang 0/0",
                        history_msgs,
                    )

                simple_prompt = "Tráº£ lá»i thÃ¢n thiá»‡n ngáº¯n gá»n (<=2 cÃ¢u) tiáº¿ng Viá»‡t cho cÃ¢u: " + message
                history_msgs = history_msgs + [
                    {"role": "user", "content": message},
                    {"role": "assistant", "content": ""},
                ]
                acc = ""
                print("DEBUG: Báº¯t Ä‘áº§u stream cÃ¢u tráº£ lá»i xÃ£ giao")
                for chunk in stream_answer(simple_prompt, temperature=float(temperature)):
                    acc += chunk
                    history_msgs[-1]["content"] = acc
                print("DEBUG: Tráº£ vá» káº¿t quáº£ stream cÃ¢u tráº£ lá»i xÃ£ giao")
                return ui_return(
                    gr.update(value=""),
                    history_msgs,
                    "(KhÃ´ng cÃ³ trÃ­ch dáº«n)",
                    acc,
                    [],
                    1,
                    "Trang 0/0",
                    history_msgs,
                )

            # Xá»­ lÃ½ cÃ¢u há»i tÃ¬m kiáº¿m luáº­t hoáº·c tráº£ lá»i phÃ¡p lÃ½
            docs: List[Dict[str, Any]] = []
            source = None

            if intent == "law_search":
                print("DEBUG: TÃ¬m kiáº¿m Ä‘iá»u luáº­t")
                docs = _fetch(intent_filters, limit=int(k)) if intent_filters else []
                source = "law_search"
                if not docs:
                    app_log.info(
                        "RÆ¡i vÃ o tÃ¬m kiáº¿m embedding",
                        extra={"__kv__": {"cau_hoi": message}},
                    )
                    docs = search_law(message, top_k=int(k), score_threshold=float(threshold))
                    source = "law_search_embedding_fallback"

            elif intent == "legal_answer":
                print("DEBUG: TÃ¬m kiáº¿m cÃ¢u tráº£ lá»i phÃ¡p lÃ½")
                docs = search_law(normalized_query, top_k=int(k), score_threshold=float(threshold))
                source = "legal_answer"

            else:
                reply = INTENT_FALLBACK_CASUAL
                history_msgs = history_msgs + [
                    {"role": "user", "content": message},
                    {"role": "assistant", "content": reply},
                ]
                print("DEBUG: Tráº£ vá» Ã½ Ä‘á»‹nh máº·c Ä‘á»‹nh")
                return ui_return(
                    gr.update(value=""),
                    history_msgs,
                    "(KhÃ´ng cÃ³ trÃ­ch dáº«n)",
                    reply,
                    [],
                    1,
                    "Trang 0/0",
                    history_msgs,
                )

            if not docs:
                reply = (
                    "ChÆ°a tÃ¬m tháº¥y cÆ¡ sá»Ÿ phÃ¡p lÃ½ phÃ¹ há»£p. "
                    "Báº¡n cÃ³ thá»ƒ bá»• sung Äiá»u/Khoáº£n hoáº·c thÃªm bá»‘i cáº£nh."
                )
                upd = history_msgs + [
                    {"role": "user", "content": message},
                    {"role": "assistant", "content": reply},
                ]
                print("DEBUG: KhÃ´ng tÃ¬m tháº¥y tÃ i liá»‡u")
                return ui_return(
                    gr.update(value=""),
                    upd,
                    "(ChÆ°a cÃ³ dá»¯ liá»‡u)",
                    reply,
                    [],
                    1,
                    "Trang 0/0",
                    upd,
                )

            if intent == "legal_answer":
                user_query = original_query or message
            elif intent == "law_search":
                user_query = message
            else:
                user_query = message
            cites_markdown, page_label = docs_page_markdown(docs, 1, int(cur_page_size))
            prompt = build_prompt(user_query, docs, history_msgs)

            log_step("llm_chuanbi", so_tai_lieu=len(docs), nguon=source)
            print(f"DEBUG: ÄÃ£ chuáº©n bá»‹ prompt, sá»‘ tÃ i liá»‡u: {len(docs)}")

            history_msgs = history_msgs + [
                {"role": "user", "content": message},
                {"role": "assistant", "content": ""},
            ]
            acc = ""
            t_llm0 = time.perf_counter()
            print("DEBUG: Báº¯t Ä‘áº§u stream cÃ¢u tráº£ lá»i phÃ¡p lÃ½")
            for chunk in stream_answer(prompt, temperature=float(temperature)):
                acc += chunk
                history_msgs[-1]["content"] = acc
            print("DEBUG: Tráº£ vá» káº¿t quáº£ stream cÃ¢u tráº£ lá»i phÃ¡p lÃ½")
            return ui_return(
                gr.update(value=""),
                history_msgs,
                cites_markdown,
                acc,
                docs,
                1,
                page_label,
                history_msgs,
            )

        except Exception as e:
            app_log.error("Lá»—i xá»­ lÃ½ cÃ¢u há»i", extra={"__kv__": {"loi": str(e)}})
            print(f"DEBUG: Lá»—i trong xá»­ lÃ½: {e}")
            return ui_return(
                gr.update(value=""),
                history_msgs,
                "(Lá»—i há»‡ thá»‘ng)",
                f"Lá»—i: {e}",
                [],
                1,
                "Trang 0/0",
                history_msgs,
            )

    # Káº¿t ná»‘i outputs
    outputs = [
        msg,
        chatbot,
        cites_md,
        state_last_answer,
        state_docs,
        state_page,
        page_info,
        state_history,
    ]
    send.click(respond, inputs=[msg, state_history, page_size], outputs=outputs, queue=False)
    msg.submit(respond, inputs=[msg, state_history, page_size], outputs=outputs, queue=False)

    # Like/Dislike
    def on_like(data: gr.LikeData):
        msg_like = data.value or {}
        role = msg_like.get("role", "assistant")
        text = msg_like.get("content", "")
        app_log.info(
            "Pháº£n há»“i ngÆ°á»i dÃ¹ng",
            extra={"__kv__": {"thich": data.liked, "vai_tro": role, "do_dai": len(text or "")}},
        )
        return None

    chatbot.like(on_like)

    # PhÃ¢n trang
    def render_cites_for_page(docs, page, cur_page_size):
        md, label = docs_page_markdown(docs or [], int(page), int(cur_page_size))
        return gr.update(value=md), int(page), label

    def go_prev(docs, page, cur_page_size):
        if not docs:
            return render_cites_for_page([], 1, cur_page_size)
        new_page = max(1, int(page) - 1)
        return render_cites_for_page(docs, new_page, cur_page_size)

    def go_next(docs, page, cur_page_size):
        if not docs:
            return render_cites_for_page([], 1, cur_page_size)
        _, total, total_pages, _ = paginate_docs(docs, 1, int(cur_page_size))
        new_page = min(total_pages if total_pages > 0 else 1, int(page) + 1)
        return render_cites_for_page(docs, new_page, cur_page_size)

    def on_change_page_size(docs, cur_page_size):
        return render_cites_for_page(docs, 1, cur_page_size)

    prev_page.click(
        go_prev,
        inputs=[state_docs, state_page, page_size],
        outputs=[cites_md, state_page, page_info],
        queue=False,
    )
    next_page.click(
        go_next,
        inputs=[state_docs, state_page, page_size],
        outputs=[cites_md, state_page, page_info],
        queue=False,
    )
    page_size.release(
        on_change_page_size,
        inputs=[state_docs, page_size],
        outputs=[cites_md, state_page, page_info],
        queue=False,
    )

    gr.Markdown(f"""
    <sub>Â© {datetime.now().year} â€” Ná»™i dung chá»‰ mang tÃ­nh tham kháº£o, khÃ´ng thay tháº¿ tÆ° váº¥n phÃ¡p lÃ½ chÃ­nh thá»©c.</sub>
    """)

if __name__ == "__main__":
    demo.queue()
    demo.launch(show_error=True, share=True)