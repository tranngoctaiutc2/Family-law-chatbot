import os
from datetime import datetime
from textwrap import dedent

import logging
import time
import re
import json
from typing import List, Dict, Any, Optional, Tuple

import gradio as gr
from dotenv import load_dotenv
from qdrant_client import QdrantClient
from sentence_transformers import SentenceTransformer
import google.generativeai as genai
from qdrant_client.http.models import Filter, FieldCondition, MatchValue
from tenacity import retry, stop_after_attempt, wait_exponential

# ================== ENV ==================
load_dotenv()
QDRANT_URL = os.getenv("QDRANT_URL", "").strip()
QDRANT_API_KEY =  os.getenv("QDRANT_API_KEY", "").strip()
COLLECTION_NAME = os.getenv("COLLECTION_NAME", "")
EMBEDDING_MODEL =  os.getenv("EMBEDDING_MODEL", "BAAI/bge-m3")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY", "").strip()
GEMINI_MODEL_ID = os.getenv("GEMINI_MODEL_ID", "models/gemini-1.5-flash")

INTENT_DEBUG = os.getenv("INTENT_DEBUG", "0").strip() in {"1", "true", "TRUE", "yes", "on"}
LOG_LEVEL = os.getenv("LOG_LEVEL", "INFO").upper()
CASUAL_MAX_WORDS = int(os.getenv("CASUAL_MAX_WORDS", "0").strip() or 0)  # 0 = khÃ´ng giá»›i háº¡n
INTENT_RAW_PREVIEW_LIMIT = int(os.getenv("INTENT_RAW_PREVIEW_LIMIT", "240").strip() or 240) #log ngáº¯n gá»n hÆ¡n
INTENT_FALLBACK_CASUAL = os.getenv(
    "INTENT_FALLBACK_CASUAL",
    "ChÃ o báº¡n, mÃ¬nh cÃ³ thá»ƒ há»— trá»£ cÃ¢u há»i vá» Luáº­t HÃ´n nhÃ¢n & Gia Ä‘Ã¬nh. Báº¡n muá»‘n há»i ná»™i dung gÃ¬?",# cÃ¢u há»i máº·c Ä‘á»‹nh khi 0 hiá»ƒu input
).strip()

if not (QDRANT_URL and QDRANT_API_KEY):
    raise RuntimeError("Thiáº¿u QDRANT_URL / QDRANT_API_KEY trong .env")

# ================== LOGGING (rÃµ rÃ ng) ==================
class KVFormatter(logging.Formatter):
    def format(self, record: logging.LogRecord) -> str:
        base = super().format(record)
        extras = []
        extras_dict = getattr(record, "__kv__", {})
        if isinstance(extras_dict, dict):
            for k, v in extras_dict.items():
                try:
                    extras.append(f"{k}={v}")
                except Exception:
                    continue
        return base + (" | " + ",".join(extras) if extras else "")

LOG_FORMAT = "%(asctime)s | %(levelname)s | %(name)s | %(message)s"
handlers = [logging.StreamHandler(), logging.FileHandler("botchat_honnhan.log", encoding="utf-8")]
for h in handlers:
    h.setFormatter(KVFormatter(LOG_FORMAT))

root_logger = logging.getLogger()
root_logger.handlers = []
root_logger.setLevel(getattr(logging, LOG_LEVEL, logging.INFO))
for h in handlers:
    root_logger.addHandler(h)

app_log = logging.getLogger("app")
metrics_logger = logging.getLogger("metrics")
if not metrics_logger.handlers:
    fh = logging.FileHandler("metrics.log", encoding="utf-8")
    fh.setFormatter(logging.Formatter("%(message)s"))
    metrics_logger.addHandler(fh)
    metrics_logger.setLevel(logging.INFO)


def log_step(event: str, **kv):
    kvpairs = ",".join([f"{k}={v}" for k, v in kv.items()])
    metrics_logger.info(f"ts={int(time.time())},evt={event},{kvpairs}")




# ================== INIT ==================
client = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY, prefer_grpc=True)
embedder = SentenceTransformer(EMBEDDING_MODEL)

genai.configure()
INTENT_SYSTEM_PROMPT = dedent("""
Báº¡n lÃ  trá»£ lÃ½ vá» Luáº­t HN&GÄ VN.
HÃ£y tráº£ vá» **JSON THUáº¦N** (khÃ´ng markdown, khÃ´ng lá»i dáº«n).

Schema má»™t trong cÃ¡c dáº¡ng:
1) {"intent":"casual","answer":"..."}
2) {"intent":"legal_answer","normalized_query":"...","original_query":"..."}
3) {"intent":"law_search","filters":{"article_no":int?,"clause_no":int?,"point_letter":str?,"chapter_number":int?}}

Quy táº¯c xÃ¡c Ä‘á»‹nh intent:
- CÃ¢u há»i nháº¯m tá»›i ná»™i dung Ä‘iá»u/khoáº£n/chÆ°Æ¡ng/má»¥c cá»¥ thá»ƒ â†’ law_search.
- XÃ£ giao/chÃ o há»i â†’ casual.
- Nháº¯c sá»‘ Ä‘iá»u/khoáº£n nhÆ°ng há»i tÃ¬nh huá»‘ng thá»±c táº¿, Ã¡p dá»¥ng, thá»§ tá»¥c â†’ legal_answer.
- LuÃ´n dá»±a vÃ o **má»¥c Ä‘Ã­ch cÃ¢u há»i**, khÃ´ng chá»‰ sá»‘ Ä‘iá»u/khoáº£n.

Náº¿u intent = casual thÃ¬ báº¯t buá»™c cÃ³ answer (tiáº¿ng Viá»‡t, lá»‹ch sá»±).
""")
gemini_model = genai.GenerativeModel(
    model_name=GEMINI_MODEL_ID,
    system_instruction=INTENT_SYSTEM_PROMPT,
)
answer_model = genai.GenerativeModel(
    model_name=GEMINI_MODEL_ID,
)
# ================== HELPERS ==================
def _safe_truncate(text: str, limit: int = 800) -> str:
    return text if text and len(text) <= limit else (text[:limit] + "â€¦(cáº¯t)") if text else ""

LEGAL_HINTS = re.compile(
    r"(?i)\b(Ä‘iá»u|khoáº£n|Ä‘iá»ƒm|chÆ°Æ¡ng|hÃ´n nhÃ¢n|ly hÃ´n|ly thÃ¢n|nuÃ´i con|tÃ i sáº£n|"
    r"quan há»‡ vá»£ chá»“ng|káº¿t hÃ´n|há»§y káº¿t hÃ´n|chung sá»‘ng nhÆ° vá»£ chá»“ng|cáº¥p dÆ°á»¡ng|giÃ¡m há»™)\b"
)

def looks_like_legal(query: str) -> bool:
    return bool(LEGAL_HINTS.search(query or ""))

class SimpleTTLCache:
    def __init__(self, ttl_seconds: int = 1800, max_items: int = 512):
        self.ttl = ttl_seconds
        self.max = max_items
        self.store: Dict[str, Tuple[float, Any]] = {}

    def _evict_if_needed(self):
        if len(self.store) <= self.max:
            return
        oldest_key = min(self.store, key=lambda k: self.store[k][0])
        self.store.pop(oldest_key, None)

    def get(self, key: str):
        item = self.store.get(key)
        if not item:
            return None
        ts, value = item
        if time.time() - ts > self.ttl:
            self.store.pop(key, None)
            return None
        return value

    def set(self, key: str, value: Any):
        self.store[key] = (time.time(), value)
        self._evict_if_needed()

embed_cache = SimpleTTLCache(ttl_seconds=3600, max_items=1024)
search_cache = SimpleTTLCache(ttl_seconds=900, max_items=1024)


def encode_query(text: str):
    key = f"{EMBEDDING_MODEL}|query|{text}"
    v = embed_cache.get(key)
    if v is not None:
        return v
    vec = embedder.encode([f"query: {text}"], normalize_embeddings=True)[0].tolist()
    embed_cache.set(key, vec)
    return vec


# ================== INTENT (xá»­ lÃ½ vÃ  phÃ¢n loáº¡i cÃ¢u há»i) ==================

def _intent_via_gemini(query: str) -> Dict[str, Any]:#AI phÃ¢n loáº¡i intent.
    try:
        cfg = genai.types.GenerationConfig(
            temperature=0.0,
            max_output_tokens=192,
            response_mime_type="application/json",
        )
        resp = gemini_model.generate_content(
            [
                {
                    "role": "user",
                    "parts": [f"CÃ¢u há»i: {query}\nHÃ£y tráº£ JSON thuáº§n phÃ¹ há»£p schema Ä‘Ã£ nÃªu."],
                }
            ],
            generation_config=cfg,
        )

        # Thu tháº­p thÃ´ng tin chi tiáº¿t vá» candidates / finish_reason / safety
        candidates = getattr(resp, "candidates", None) or []
        first_cand = candidates[0] if candidates else None
        finish_reason = getattr(first_cand, "finish_reason", None)
        safety = []
        try:
            if first_cand and getattr(first_cand, "safety_ratings", None):
                for s in first_cand.safety_ratings:
                    cat = getattr(s, "category", "")
                    prob = getattr(s, "probability", "")
                    safety.append(f"{cat}:{prob}")
        except Exception:
            pass

        raw = getattr(resp, "text", "") or ""
        app_log.info(
            "INTENT_RAW",
            extra={
                "__kv__": {
                    "len": len(raw),
                    "preview": _safe_truncate(raw, INTENT_RAW_PREVIEW_LIMIT),
                    "candidates": len(candidates),
                    "finish_reason": finish_reason,
                    "safety": ";".join(safety[:6]),
                }
            },
        )

        # Náº¿u bá»‹ block (finish_reason == 2) hoáº·c khÃ´ng cÃ³ text => fallback casual trá»±c tiáº¿p
        if finish_reason == 2 or not raw:
            log_step("intent_block", reason=str(finish_reason))
            app_log.warning(
                "INTENT_BLOCKED", extra={"__kv__": {"finish_reason": finish_reason, "raw_len": len(raw)}}
            )
            return {"intent": "casual", "answer": INTENT_FALLBACK_CASUAL}

        data = json.loads(raw) if raw else {}
        if not isinstance(data, dict):
            app_log.warning("INTENT_NON_DICT")
            return {"intent": "casual", "answer": INTENT_FALLBACK_CASUAL}
        out: Dict[str, Any] = {}
        for k in ("intent", "answer", "normalized_query", "filters", "original_query"):
            if k in data and data[k] not in (None, ""):
                out[k] = data[k]
        app_log.info(
            "INTENT_PARSED",
            extra={
                "__kv__": {
                    "intent": out.get("intent", ""),
                    "has_answer": int("answer" in out and bool(out.get("answer"))),
                    "has_filters": int("filters" in out and bool(out.get("filters"))),
                    "ans_len": len(out.get("answer", "") or ""),
                    "finish_reason": finish_reason,
                }
            },
        )
        return out
    except Exception as e:
        app_log.warning("INTENT_ERR", extra={"__kv__": {"err": str(e)}})
        # Fallback cuá»‘i cÃ¹ng khi exception báº¥t thÆ°á»ng
        return {"intent": "casual", "answer": INTENT_FALLBACK_CASUAL}


def analyze_intent(query: str) -> Dict[str, Any]:#bá»™ lá»c + fallback thá»§ cÃ´ng Ä‘á»ƒ cháº¯c cháº¯n lÃºc nÃ o cÅ©ng cÃ³ intent há»£p lá»‡.
    data = _intent_via_gemini(query)
    intent = data.get("intent")
    answer = data.get("answer", "")
    normalized_query = data.get("normalized_query", "") or query
    original_query = data.get("original_query", "") 
    filters = data.get("filters", {}) or {}

    if intent not in {"casual", "legal_answer", "law_search"}:
        if re.search(r"(?i)\bÄ‘iá»u\s*\d+", query) or re.search(r"(?i)\bkhoáº£n\s*\d+", query):
            intent = "law_search"
        elif looks_like_legal(query):
            intent = "legal_answer"
        else:
            intent = "casual"
        log_step("intent_fallback", qlen=len(query))

    log_step("intent", loai=intent, co_legal=str(looks_like_legal(query)))
    app_log.info("INTENT_DECISION", extra={"__kv__": {"intent": intent}})
    return {"intent": intent, "answer": answer, "normalized_query": normalized_query, "original_query": original_query, "filters": filters}


# ================== HIBRID SEARCH ==================


def _build_filter(query_text: str) -> Optional[Filter]:
    conds: List[FieldCondition] = []
    m = re.search(r"(?i)\bÄ‘iá»u\s*(\d+)\b", query_text)
    if m:
        conds.append(FieldCondition(key="article_no", match=MatchValue(value=int(m.group(1)))))
    m = re.search(r"(?i)\bkhoáº£n\s*(\d+)\b", query_text)
    if m:
        conds.append(FieldCondition(key="clause_no", match=MatchValue(value=int(m.group(1)))))
    m = re.search(r"(?i)\bÄ‘iá»ƒm\s*([a-z])\b", query_text)
    if m:
        conds.append(FieldCondition(key="point_letter", match=MatchValue(value=m.group(1).lower())))
    m = re.search(r"(?i)\bchÆ°Æ¡ng\s*(\d+)\b", query_text)
    if m:
        conds.append(FieldCondition(key="chapter_number", match=MatchValue(value=int(m.group(1)))))
    return Filter(must=conds) if conds else None


def search_law(query: str, top_k: int = 15, score_threshold: float = 0.42):
    t0 = time.perf_counter()
    app_log.info("SEARCH_BEGIN", extra={"__kv__": {"q": _safe_truncate(query, 80), "k": top_k, "thr": score_threshold}})

    cache_key = f"search|{COLLECTION_NAME}|{top_k}|{score_threshold}|{query}"
    cached = search_cache.get(cache_key)
    if cached is not None:
        app_log.info("SEARCH_CACHE_HIT")
        return cached

    try:
        t_embed0 = time.perf_counter()
        vec = encode_query(query)
        t_embed = time.perf_counter() - t_embed0

        flt = _build_filter(query)

        t_q0 = time.perf_counter()
        results = client.search(
            collection_name=COLLECTION_NAME,
            query_vector=vec,
            with_payload=True,
            limit=max(1, min(int(top_k) * 3, 80)),
            query_filter=flt
        )
        t_qdrant = time.perf_counter() - t_q0

        raw_docs = []
        for r in results:
            p = r.payload or {}
            raw_docs.append({
                "citation": p.get("exact_citation", ""),
                "chapter_number": p.get("chapter_number", ""),
                "article_no": p.get("article_no", ""),
                "article_title": p.get("article_title", ""),
                "clause_no": p.get("clause_no", ""),
                "point_letter": p.get("point_letter", ""),
                "content": (p.get("content") or "").strip(),
                "score": float(r.score or 0.0),
            })

        t_filter0 = time.perf_counter()
        filtered = [d for d in raw_docs if d.get("score", 0.0) >= score_threshold] or raw_docs[:max(1, top_k)]
        t_filter = time.perf_counter() - t_filter0

        selected = filtered[:top_k]
        search_cache.set(cache_key, selected)

        sk_top1 = selected[0]['score'] if selected else 0.0
        log_step(
            "tim_kiem",
            k_yeu_cau=top_k,
            k_tra=f"{len(selected)}",
            sk_top1=f"{sk_top1:.4f}",
            t_embed=f"{t_embed:.4f}",
            t_qdrant=f"{t_qdrant:.4f}",
            t_loc=f"{t_filter:.4f}",
            t_tong=f"{time.perf_counter()-t0:.4f}"
        )
        app_log.info("SEARCH_DONE", extra={"__kv__": {"count": len(selected), "top1": f"{sk_top1:.4f}"}})
        return selected
    except Exception as e:
        app_log.error("SEARCH_ERR", extra={"__kv__": {"err": str(e)}})
        log_step("tim_kiem_loi", msg=str(e))
        raise

# ================== RENDER UTILS ==================

def law_line(d: Dict[str, Any]) -> Tuple[str, str, str]:
    art = d.get("article_no"); cls = d.get("clause_no"); pt  = d.get("point_letter")
    parts = []
    if pt: parts.append(f"Äiá»ƒm {pt}")
    if cls: parts.append(f"khoáº£n {cls}")
    if art: parts.append(f"Ä‘iá»u {art}")
    cited = " ".join(parts)
    chapter = f" (ChÆ°Æ¡ng {d.get('chapter_number')})" if d.get("chapter_number") else ""
    title = f" â€” {d.get('article_title')}" if d.get("article_title") else ""
    return cited, chapter, title


def docs_to_markdown(docs: List[Dict[str, Any]]):
    if not docs:
        return "âŒ KhÃ´ng tÃ¬m tháº¥y Ä‘iá»u luáº­t nÃ o."
    lines = []
    for i, d in enumerate(docs, 1):
        cited, chapter, title = law_line(d)
        content = (d.get("content") or "").strip()
        score = round(d.get("score", 0.0), 4)
        lines.append(
            f"**{i}.  {cited}{chapter}{title}**  \n" +
            f"{content}  \n" +
            f"<sub>Äá»™ liÃªn quan: {score}</sub>\n"
        )
    return "\n".join(lines)

def paginate_docs(docs, page: int, page_size: int):
    total = len(docs)
    if total == 0:
        return [], 0, 0, 0
    page = max(1, int(page))
    page_size = max(1, int(page_size))
    start = (page - 1) * page_size
    end = start + page_size
    sliced = docs[start:end]
    total_pages = (total + page_size - 1) // page_size
    return sliced, total, total_pages, start

def docs_page_markdown(docs, page: int, page_size: int):
    sliced, total, total_pages, start = paginate_docs(docs, page, page_size)
    if total == 0:
        return "(ChÆ°a cÃ³ dá»¯ liá»‡u)", "Trang 0/0"
    body = docs_to_markdown(sliced)
    page_label = f"Trang {page}/{total_pages} â€” hiá»ƒn thá»‹ {start+1}â€“{min(start+len(sliced), total)} / {total}"
    return f"**{page_label}**\n\n{body}", page_label


# ================== PROMPT ==================
def build_prompt(query: str, docs: List[Dict[str, Any]], history_msgs=None, ):
    
    history_block = ""
    if history_msgs:
        lines = []
        for i, m in enumerate(history_msgs[-5:], 1):
            role = m.get("role", "")
            content = m.get("content", "")
            role_label = "NgÆ°á»i dÃ¹ng" if role == "user" else "Trá»£ lÃ½"
            lines.append(f"- {i}. {role_label}: {content}")
        history_block = "\nLá»‹ch sá»­ há»™i thoáº¡i gáº§n Ä‘Ã¢y:\n" + "\n".join(lines)

    docs_sorted = sorted(
        docs,
        key=lambda d: (
            int(d.get("article_no") or 9999),
            int(d.get("clause_no") or 9999),
            str(d.get("point_letter") or "")
        )
    )

    context_lines = []
    for idx, d in enumerate(docs_sorted, 1):
        art = d.get("article_no"); cls = d.get("clause_no"); pt  = d.get("point_letter")
        parts = []
        if pt: parts.append(f"Äiá»ƒm {pt}")
        if cls: parts.append(f"khoáº£n {cls}")
        if art: parts.append(f"Ä‘iá»u {art}")
        cited = " ".join(parts)
        chapter = f" (ChÆ°Æ¡ng {d.get('chapter_number')})" if d.get("chapter_number") else ""
        title = f" â€” {d.get('article_title')}" if d.get("article_title") else ""
        content = (d.get("content") or "").strip()
        context_lines.append(f"{idx}) {cited}{chapter}{title}: {content}")

    context = "\n".join(context_lines) if context_lines else "âŒ KhÃ´ng cÃ³ Ä‘iá»u luáº­t nÃ o."

    prompt = dedent(f"""
    Báº¡n lÃ  luáº­t sÆ° tÆ° váº¥n vá» Luáº­t HÃ´n nhÃ¢n & Gia ÄÃ¬nh. Chá»‰ dÃ¹ng cÃ¡c trÃ­ch Ä‘oáº¡n trong danh sÃ¡ch dÆ°á»›i Ä‘Ã¢y 
    Quy táº¯c:
    - Náº¿u cÃ¢u há»i lÃ  nháº­n Ä‘á»‹nh ÄÃºng/Sai â†’ tráº£ lá»i **Káº¿t luáº­n: ÄÃºng/Sai** + lÃ½ do.
    - Náº¿u cÃ¢u há»i thÆ°á»ng â†’ tráº£ lá»i **ngáº¯n gá»n 1â€“3 cÃ¢u**, bÃ¡m sÃ¡t cÃ¢u há»i.
    - **TrÃ­ch dáº«n nguyÃªn vÄƒn** cÃ¡c Ä‘iá»u luáº­t liÃªn quan trong danh sÃ¡ch (khÃ´ng bá» sÃ³t), theo thá»© tá»±: Äiá»ƒm â€“ Khoáº£n â€“ Äiá»u + ná»™i dung.
    - Náº¿u thiáº¿u cÄƒn cá»©, tráº£ lá»i: **â€œKhÃ´ng Ä‘á»§ cÄƒn cá»©.â€**
    - Náº¿u cÃ¢u há»i khÃ´ng liÃªn quan Ä‘áº¿n luáº­t â†’ tráº£ lá»i lá»‹ch sá»±, ngáº¯n gá»n, khÃ´ng viá»‡n dáº«n luáº­t.
    Äá»ŠNH Dáº NG TRáº¢ Lá»œI:
    - TrÃ­ch dáº«n: <liá»‡t kÃª toÃ n bá»™ Äiá»ƒmâ€“Khoáº£nâ€“Äiá»u + ná»™i dung nguyÃªn vÄƒn>
    - Giáº£i thÃ­ch: <1â€“3 cÃ¢u, Ã¡p dá»¥ng vÃ o tÃ¬nh huá»‘ng>
    - Káº¿t luáº­n: <káº¿t luáº­n ngáº¯n gá»n dá»±a vÃ o cÃ¢u há»i vÃ  giáº£i thÃ­ch>


    CÃ¢u há»i hiá»‡n táº¡i:
    \"\"\"{query}\"\"\"{history_block}

    Danh sÃ¡ch Ä‘iá»u luáº­t (top_k):
    {context}
    """).strip()

    return prompt

# ================== LLM STREAM ==================

@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))
def _gemini_stream(prompt, temperature: float):
    cfg = genai.types.GenerationConfig(temperature=float(temperature))
    return answer_model.generate_content(prompt, generation_config=cfg, stream=True)


def stream_answer(prompt, temperature=0.2):
    t0 = time.perf_counter(); t_first0 = time.perf_counter(); first_token_emitted = False
    try:
        resp = _gemini_stream(prompt, temperature)
        for ch in resp:
            if getattr(ch, "text", None):
                if not first_token_emitted:
                    log_step("llm_first_token", do_truoc=f"{time.perf_counter()-t_first0:.4f}")
                    first_token_emitted = True
                yield ch.text
    except Exception as e:
        app_log.error("LLM_ERR", extra={"__kv__": {"err": str(e)}})
        yield f"\n\nLá»—i gá»i mÃ´ hÃ¬nh: {e}"
    finally:
        log_step("llm_tong", t=f"{time.perf_counter()-t0:.4f}")

# -------- Qdrant Fetch Helper --------
def _fetch(filters: Dict[str, Any], limit: int = 10):
    must = []
    mapping = {"article_no": int, "clause_no": int, "point_letter": str, "chapter_number": int}
    for k, caster in mapping.items():
        if k in filters and filters[k] not in (None, ""):
            try:
                val = caster(filters[k])
                must.append(FieldCondition(key=k, match=MatchValue(value=val)))
            except Exception as e:
                app_log.warning("FETCH_CAST_ERROR", extra={"__kv__": {"key": k, "value": filters[k], "error": str(e)}})
                try:
                    val = str(filters[k])
                    must.append(FieldCondition(key=k, match=MatchValue(value=val)))
                except:
                    pass
    app_log.info("FETCH_FILTERS", extra={"__kv__": {"raw_filters": str(filters), "must_conditions": str(must)}})
    if not must:
        app_log.warning("FETCH_NO_MUST")
        return []
    flt = Filter(must=must)
    out = []
    try:
        scroll_res, _ = client.scroll(
            collection_name=COLLECTION_NAME,
            scroll_filter=flt,
            limit=min(64, max(5, limit)),
            with_payload=True
        )
        app_log.info("FETCH_SCROLL_RESULTS", extra={"__kv__": {"count": len(scroll_res), "collection": COLLECTION_NAME}})
        for r in scroll_res:
            p = r.payload or {}
            out.append({
                "chapter": p.get("chapter", ""),
                "article_no": p.get("article_no", ""),
                "article_title": p.get("article_title", ""),
                "clause_no": p.get("clause_no", ""),
                "point_letter": p.get("point_letter", ""),
                "content": (p.get("content") or "").strip(),
                "score": 1.0,
            })
            if len(out) >= limit:
                break
    except Exception as e:
        app_log.error("FETCH_SCROLL_ERROR", extra={"__kv__": {"error": str(e), "collection": COLLECTION_NAME}})
        return []
    if not out:
        app_log.warning("FETCH_NO_OUT", extra={"__kv__": {"filters": str(filters)}})
    return out

# ================== UI (tá»‘i giáº£n, Ä‘Ãºng state) ==================
# CSS
CSS = """
#chatbot { height: 540px !important; }
label { font-size:12px !important; opacity:.9 }
#cites-box { 
    max-height: 360px; 
    overflow-y: auto; 
    border: 1px solid #ddd; 
    padding: 6px; 
    border-radius: 6px;
    background-color: #fafafa;
}
#history_box { 
    max-height: 200px;
}
"""

with gr.Blocks(
    title="âš–ï¸ Trá»£ lÃ½ Luáº­t HN&GÄ 2014",
    css=CSS,
) as demo:
    gr.Markdown("""
    ### âš–ï¸ Trá»£ lÃ½ Luáº­t HÃ´n NhÃ¢n & Gia ÄÃ¬nh 2014
    *Tham chiáº¿u chÃ­nh xÃ¡c â€¢ Háº¡n cháº¿ suy diá»…n â€¢ KhÃ´ng thay tháº¿ tÆ° váº¥n phÃ¡p lÃ½*
    """)

    with gr.Row():
        with gr.Column(scale=7):
            chatbot = gr.Chatbot(
                value=[], type="messages", show_copy_button=True, elem_id="chatbot"
            )
            with gr.Row():
                ex1 = gr.Button("ChÃ o báº¡n")
                ex2 = gr.Button("Äiá»u 81 quy Ä‘á»‹nh gÃ¬ vá» viá»‡c nuÃ´i con sau ly hÃ´n")
                ex3 = gr.Button("Khoáº£n 2 Äiá»u 56 nÃ³i gÃ¬")
        with gr.Column(scale=5):
            history_box = gr.Chatbot(
                value=[],
                type="messages",
                show_copy_button=False,
                label="ğŸ“œ Lá»‹ch sá»­ chat",
                elem_id="history_box"
            )
            gr.Markdown("**CÆ¡ sá»Ÿ phÃ¡p lÃ½**")
            cites_md = gr.Markdown(value="(ChÆ°a cÃ³ dá»¯ liá»‡u)", elem_id="cites-box")
            with gr.Row():
                prev_page = gr.Button("â¬…ï¸")
                next_page = gr.Button("â¡ï¸")
            with gr.Row():
                page_info = gr.Markdown("Trang 0/0")
                page_size = gr.Slider(3, 20, value=5, step=1, label="Má»—i trang")

    with gr.Row():
        msg = gr.Textbox(placeholder="Nháº­p cÃ¢u há»i...", scale=5, autofocus=True)
        send = gr.Button("Gá»­i", variant="primary", scale=1)
        clear = gr.Button("LÃ m má»›i", scale=1)

    # Prefill examples
    def _fill(text):
        return text
    ex1.click(lambda: _fill("ChÃ o báº¡n"), outputs=msg)
    ex2.click(lambda: _fill("Äiá»u 81 quy Ä‘á»‹nh gÃ¬ vá» viá»‡c nuÃ´i con sau ly hÃ´n"), outputs=msg)
    ex3.click(lambda: _fill("Khoáº£n 2 Äiá»u 56 nÃ³i gÃ¬"), outputs=msg)

    # States
    state_history = gr.State([])
    state_last_answer = gr.State("")
    state_docs = gr.State([])
    state_page = gr.State(1)

    # Helper Ä‘á»ƒ Ä‘áº£m báº£o Ä‘Ãºng thá»© tá»±/Ä‘á»§ outputs
    def ui_return(msg_val, chatbot_val, cites_val, last_answer_val, docs_val, page_val, page_label_val, history_val):
        return (
            msg_val, chatbot_val, gr.update(value=cites_val), last_answer_val,
            docs_val, page_val, page_label_val, history_val
        )

    
        # -------- Core Handler (Streaming) --------
    def respond(message, history_msgs, cur_page_size, k=15 , temperature=0.2, threshold=0.42):
        if not (message and message.strip()):
            gr.Info("Vui lÃ²ng nháº­p cÃ¢u há»i.")
            return ui_return(gr.update(), history_msgs, "", "", [], 1, "Trang 0/0", history_msgs)

        t_overall0 = time.perf_counter()
        try:
            # ---------- intent detection (Gemini láº§n 1, chá»‰ ná»™i bá»™) ----------
            intent_info = analyze_intent(message)
            intent = intent_info["intent"]
            intent_answer = intent_info.get("answer", "")
            normalized_query = intent_info.get("normalized_query", message)
            original_query = intent_info.get("original_query", message)
            intent_filters = intent_info.get("filters", {})

            # ===== CASUAL =====
            if intent == "casual":
                final_answer = (intent_answer or "").replace("\u200b", "").strip()
                app_log.info("CASUAL_BRANCH", extra={"__kv__": {"ans_len": len(final_answer)}})

                # Giá»›i háº¡n sá»‘ tá»« náº¿u cáº¥u hÃ¬nh
                if final_answer and CASUAL_MAX_WORDS > 0:
                    words = final_answer.split()
                    if len(words) > CASUAL_MAX_WORDS:
                        truncated = " ".join(words[:CASUAL_MAX_WORDS])
                        app_log.info(
                            "CASUAL_TRUNCATE",
                            extra={"__kv__": {"orig_words": len(words), "kept": CASUAL_MAX_WORDS, "orig_len": len(final_answer)}},
                        )
                        final_answer = truncated

                # Náº¿u Gemini Ä‘Ã£ tráº£ sáºµn answer â†’ dÃ¹ng luÃ´n
                if len(final_answer) >= 1:
                    history_msgs = history_msgs + [
                        {"role": "user", "content": message},
                        {"role": "assistant", "content": final_answer},
                    ]
                    yield ui_return(gr.update(value=""), history_msgs, "(KhÃ´ng cÃ³ trÃ­ch dáº«n)", final_answer, [], 1, "Trang 0/0", history_msgs)
                    log_step("hoan_tat", t_tong=f"{time.perf_counter()-t_overall0:.4f}", t_llm="casual_direct")
                    return

                # Náº¿u khÃ´ng cÃ³ answer â†’ fallback gá»i Gemini stream ngáº¯n
                simple_prompt = "Tráº£ lá»i thÃ¢n thiá»‡n ngáº¯n gá»n (<=2 cÃ¢u) tiáº¿ng Viá»‡t cho cÃ¢u: " + message
                history_msgs = history_msgs + [
                    {"role": "user", "content": message},
                    {"role": "assistant", "content": ""},
                ]
                acc = ""
                for chunk in stream_answer(simple_prompt, temperature=float(temperature)):
                    acc += chunk
                    history_msgs[-1]["content"] = acc
                    yield ui_return(gr.update(value=""), history_msgs, "(KhÃ´ng cÃ³ trÃ­ch dáº«n)", acc, [], 1, "Trang 0/0", history_msgs)
                log_step("hoan_tat", t_tong=f"{time.perf_counter()-t_overall0:.4f}", t_llm="casual_stream")
                return

            # ===== LAW_SEARCH & LEGAL_ANSWER =====
            docs: List[Dict[str, Any]] = []
            source = None

            if intent == "law_search":
                docs = _fetch(intent_filters, limit=int(k)) if intent_filters else []
                source = "law_search"

            elif intent == "legal_answer":
                docs = search_law(normalized_query, top_k=int(k), score_threshold=float(threshold))
                source = "legal_answer"

            else:
                # fallback: intent khÃ´ng xÃ¡c Ä‘á»‹nh
                reply = INTENT_FALLBACK_CASUAL
                history_msgs = history_msgs + [
                    {"role": "user", "content": message},
                    {"role": "assistant", "content": reply},
                ]
                yield ui_return(gr.update(value=""), history_msgs, "(KhÃ´ng cÃ³ trÃ­ch dáº«n)", reply, [], 1, "Trang 0/0", history_msgs)
                return

            # Náº¿u khÃ´ng tÃ¬m tháº¥y docs
            if not docs:
                reply = "ChÆ°a tÃ¬m tháº¥y cÆ¡ sá»Ÿ phÃ¡p lÃ½ phÃ¹ há»£p. Báº¡n cÃ³ thá»ƒ bá»• sung Äiá»u/Khoáº£n hoáº·c thÃªm bá»‘i cáº£nh."
                upd = history_msgs + [
                    {"role": "user", "content": message},
                    {"role": "assistant", "content": reply},
                ]
                yield ui_return(gr.update(value=""), upd, "(ChÆ°a cÃ³ dá»¯ liá»‡u)", reply, [], 1, "Trang 0/0", upd)
                return

            # Chuáº©n bá»‹ citations + prompt (cho Gemini láº§n 2)
            if intent == "legal_answer":
                user_query = original_query or message
            elif intent == "law_search":
                user_query = message
            else:
                user_query = message
            cites_markdown, page_label = docs_page_markdown(docs, 1, int(cur_page_size))
            prompt = build_prompt(user_query, docs, history_msgs)
            
            log_step("llm_chuanbi", k_docs=len(docs), source=source)

            # Stream Gemini answer (Gemini láº§n 2, tráº£ ra UI)
            history_msgs = history_msgs + [
                {"role": "user", "content": message},
                {"role": "assistant", "content": ""},
            ]
            acc = ""
            t_llm0 = time.perf_counter()
            for chunk in stream_answer(prompt, temperature=float(temperature)):
                acc += chunk
                history_msgs[-1]["content"] = acc
                yield ui_return(gr.update(value=""), history_msgs, cites_markdown, acc, docs, 1, page_label, history_msgs)

            log_step("hoan_tat", source=source, t_tong=f"{time.perf_counter()-t_overall0:.4f}", t_llm=f"{time.perf_counter()-t_llm0:.4f}")
            return 

        except Exception as e:
            app_log.error("RESPOND_ERR", extra={"__kv__": {"err": str(e)}})
            return ui_return(gr.update(value=""), history_msgs, "(Lá»—i há»‡ thá»‘ng)", f"Lá»—i: {e}", [], 1, "Trang 0/0", history_msgs)


    # Wiring outputs
    outputs = [msg, chatbot, cites_md, state_last_answer, state_docs, state_page, page_info, state_history]
    send.click(respond, inputs=[msg, state_history,page_size], outputs=outputs, queue=True)
    msg.submit(respond, inputs=[msg, state_history, page_size], outputs=outputs, queue=True)


    # Like/Dislike
    def on_like(data: gr.LikeData):
        msg_like = data.value or {}
        role = msg_like.get("role", "assistant"); text = msg_like.get("content", "")
        app_log.info("FEEDBACK", extra={"__kv__": {"liked": data.liked, "role": role, "len": len(text or "")}})
        return None
    chatbot.like(on_like)

    # Pagination
    def render_cites_for_page(docs, page, cur_page_size):
        md, label = docs_page_markdown(docs or [], int(page), int(cur_page_size))
        return gr.update(value=md), int(page), label

    def go_prev(docs, page, cur_page_size):
        if not docs:
            return render_cites_for_page([], 1, cur_page_size)
        new_page = max(1, int(page) - 1)
        return render_cites_for_page(docs, new_page, cur_page_size)

    def go_next(docs, page, cur_page_size):
        if not docs:
            return render_cites_for_page([], 1, cur_page_size)
        _, total, total_pages, _ = paginate_docs(docs, 1, int(cur_page_size))
        new_page = min(total_pages if total_pages > 0 else 1, int(page) + 1)
        return render_cites_for_page(docs, new_page, cur_page_size)

    def on_change_page_size(docs, cur_page_size):
        return render_cites_for_page(docs, 1, cur_page_size)

    prev_page.click(go_prev, inputs=[state_docs, state_page, page_size], outputs=[cites_md, state_page, page_info], queue=False)
    next_page.click(go_next, inputs=[state_docs, state_page, page_size], outputs=[cites_md, state_page, page_info], queue=False)
    page_size.release(on_change_page_size, inputs=[state_docs, page_size], outputs=[cites_md, state_page, page_info], queue=False)

    gr.Markdown(f"""
    <sub>Â© {datetime.now().year} â€” Ná»™i dung chá»‰ mang tÃ­nh tham kháº£o, khÃ´ng thay tháº¿ tÆ° váº¥n phÃ¡p lÃ½ chÃ­nh thá»©c.</sub>
    """)

if __name__ == "__main__":
    demo.launch(show_error=True)