import os
from datetime import datetime
from textwrap import dedent

import gradio as gr
from dotenv import load_dotenv
from qdrant_client import QdrantClient
from sentence_transformers import SentenceTransformer
import google.generativeai as genai
import uuid
from memory import get_memory, get_history_messages, clear_history

# ================== ENV ==================
load_dotenv()
QDRANT_URL = os.getenv("QDRANT_URL", "").strip()
QDRANT_API_KEY =  os.getenv("QDRANT_API_KEY", "").strip()
COLLECTION_NAME = os.getenv("COLLECTION_NAME", "")
EMBEDDING_MODEL =  os.getenv("EMBEDDING_MODEL", "BAAI/bge-m3")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY", "").strip()
GEMINI_MODEL_ID = os.getenv("GEMINI_MODEL_ID", "models/gemini-1.5-flash")

if not (QDRANT_URL and QDRANT_API_KEY and GEMINI_API_KEY):
    raise RuntimeError("Thi·∫øu QDRANT_URL / QDRANT_API_KEY / GEMINI_API_KEY trong .env")

# ================== INIT ==================
client = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY, prefer_grpc=True)
embedder = SentenceTransformer(EMBEDDING_MODEL)

genai.configure(api_key=GEMINI_API_KEY)
gemini_model = genai.GenerativeModel(GEMINI_MODEL_ID)

# ================== SEARCH HELPERS ==================
def search_law(query: str, top_k: int = 7):
    """
    T√¨m ki·∫øm trong Qdrant v√† tr·∫£ v·ªÅ danh s√°ch ƒëi·ªÅu lu·∫≠t + ƒëi·ªÉm t∆∞∆°ng ƒë·ªìng.
    V·ªõi BAAI/bge-m3 n√™n normalize ƒë·ªÉ k·∫øt qu·∫£ ·ªïn ƒë·ªãnh.
    """
    vec = embedder.encode([query], normalize_embeddings=True)[0].tolist()
    try:
        results = client.query_points(
            collection_name=COLLECTION_NAME,
            query=vec,
            limit=max(1, min(int(top_k), 50)),
            with_payload=True
        ).points
    except Exception as e:
        print(f"[ERROR] Qdrant query failed: {e}")
        return []

    docs = []
    for r in results:
        p = r.payload or {}
        docs.append({
            "citation": p.get("exact_citation", ""),
            "chapter": p.get("chapter", ""),
            "article_no": p.get("article_no", ""),
            "article_title": p.get("article_title", ""),
            "clause_no": p.get("clause_no", ""),
            "point_letter": p.get("point_letter", ""),
            "content": (p.get("content") or "").strip(),
            "score": float(r.score or 0.0),
        })
    return docs

def law_line(d):
    cited = d.get("citation") or (
        f"ƒêi·ªÅu {d.get('article_no','')}"
        + (f", Kho·∫£n {d.get('clause_no')}" if d.get('clause_no') else "")
        + (f", ƒêi·ªÉm {d.get('point_letter')}" if d.get('point_letter') else "")
    )
    chapter = f" ({d.get('chapter')})" if d.get("chapter") else ""
    title = f" ‚Äî {d.get('article_title')}" if d.get("article_title") else ""
    return cited, chapter, title

def docs_to_markdown(docs):
    """
    Hi·ªÉn th·ªã Top-K ƒëi·ªÅu lu·∫≠t ·ªü d·∫°ng Markdown (tr√°nh l·ªói [Object Object]).
    """
    if not docs:
        return "‚ùå Kh√¥ng t√¨m th·∫•y ƒëi·ªÅu lu·∫≠t n√†o."
    lines = []
    for i, d in enumerate(docs, 1):
        cited, chapter, title = law_line(d)
        content = (d.get("content") or "").strip()
        score = round(d.get("score", 0.0), 4)
        lines.append(
            f"**{i}. {cited}{chapter}{title}**  \n"
            f"{content}  \n"
            f"<sub>ƒê·ªô li√™n quan: {score}</sub>\n"
        )
    return "\n".join(lines)

# -------- Ph√¢n trang cho c∆° s·ªü ph√°p l√Ω --------
def paginate_docs(docs, page: int, page_size: int):
    total = len(docs)
    if total == 0:
        return [], 0, 0, 0
    page = max(1, int(page))
    page_size = max(1, int(page_size))
    start = (page - 1) * page_size
    end = start + page_size
    sliced = docs[start:end]
    total_pages = (total + page_size - 1) // page_size
    return sliced, total, total_pages, start

def docs_page_markdown(docs, page: int, page_size: int):
    sliced, total, total_pages, start = paginate_docs(docs, page, page_size)
    if total == 0:
        return "(Ch∆∞a c√≥ d·ªØ li·ªáu)", "Trang 0/0"
    body = docs_to_markdown(sliced)
    page_label = f"Trang {page}/{total_pages} ‚Äî hi·ªÉn th·ªã {start+1}‚Äì{min(start+len(sliced), total)} / {total}"
    return f"**{page_label}**\n\n{body}", page_label

# ================== CLASSIFY: c√≥ li√™n quan ph√°p l√Ω? ==================
def is_legal_query(user_query: str) -> bool:
    """
    D√πng Gemini ƒë·ªÉ ph√¢n lo·∫°i nhanh xem c√¢u h·ªèi c√≥ *li√™n quan t·ªõi ph√°p l√Ω h√¥n nh√¢n & gia ƒë√¨nh VN 2014* hay kh√¥ng.
    Tr·∫£ v·ªÅ True n·∫øu LI√äN QUAN, False n·∫øu kh√¥ng.
    """
    prompt = dedent(f"""
    H√£y ph√¢n lo·∫°i c√¢u sau c√≥ LI√äN QUAN ƒë·∫øn t∆∞ v·∫•n ph√°p l√Ω theo Lu·∫≠t H√¥n nh√¢n & Gia ƒë√¨nh Vi·ªát Nam 2014 hay kh√¥ng.

    Y√äU C·∫¶U:
    - N·∫øu LI√äN QUAN: tr·∫£ v·ªÅ ƒë√∫ng m·ªôt t·ª´ "LEGAL".
    - N·∫øu KH√îNG LI√äN QUAN (x√£ giao, chitchat, th·ªùi ti·∫øt, c√¥ng ngh·ªá, c√°c lu·∫≠t kh√°c...): tr·∫£ v·ªÅ ƒë√∫ng m·ªôt t·ª´ "NONLEGAL".
    - Kh√¥ng th√™m l·ªùi gi·∫£i th√≠ch.

    C√ÇU C·∫¶N PH√ÇN LO·∫†I:
    ---
    {user_query}
    ---
    """).strip()

    try:
        cfg = genai.types.GenerationConfig(temperature=0.0)
        resp = gemini_model.generate_content(prompt, generation_config=cfg)
        text = (getattr(resp, "text", None) or "").strip().upper()
        if "LEGAL" in text and "NON" not in text:
            return True
        if text == "NONLEGAL" or "NONLEGAL" in text:
            return False
        # N·∫øu model tr·∫£ r√°c, fallback heuristic ƒë∆°n gi·∫£n theo t·ª´ kh√≥a ph√°p l√Ω
        keywords = [
            "ly h√¥n", "ly hon", "k·∫øt h√¥n", "ket hon", "h√¥n nh√¢n", "hon nhan",
            "con chung", "nu√¥i con", "cap duong", "c·∫•p d∆∞·ª°ng", "t√†i s·∫£n chung",
            "chia t√†i s·∫£n", "gi√†nh quy·ªÅn", "gi√°m h·ªô", "giam ho",
            "ly th√¢n", "ly than", "ƒëi·ªÅu", "kho·∫£n", "ƒëi·ªÉm", "to√†", "t√≤a", "to√† √°n", "t√≤a √°n",
            "gi·∫•y ƒëƒÉng k√Ω k·∫øt h√¥n", "h·ªßy k·∫øt h√¥n", "c·∫•m k·∫øt h√¥n"
        ]
        q = user_query.lower()
        return any(k in q for k in keywords)
    except Exception:
        # L·ªói g·ªçi model => d√πng heuristic
        keywords = [
            "ly h√¥n", "ly hon", "k·∫øt h√¥n", "ket hon", "h√¥n nh√¢n", "hon nhan",
            "con chung", "nu√¥i con", "cap duong", "c·∫•p d∆∞·ª°ng", "t√†i s·∫£n chung",
            "chia t√†i s·∫£n", "gi√†nh quy·ªÅn", "gi√°m h·ªô", "giam ho",
            "ly th√¢n", "ly than", "ƒëi·ªÅu", "kho·∫£n", "ƒëi·ªÉm", "to√†", "t√≤a", "to√† √°n", "t√≤a √°n",
            "gi·∫•y ƒëƒÉng k√Ω k·∫øt h√¥n", "h·ªßy k·∫øt h√¥n", "c·∫•m k·∫øt h√¥n"
        ]
        q = user_query.lower()
        return any(k in q for k in keywords)

# ================== PROMPT ==================
def build_prompt(query: str, docs, history_msgs, law_name="Lu·∫≠t H√¥n nh√¢n v√† Gia ƒë√¨nh 2014"):
    # L·∫•y t·ªëi ƒëa 5 l∆∞·ª£t h·ªôi tho·∫°i g·∫ßn nh·∫•t
    history_block = ""
    if history_msgs:
        lines = []
        for i, m in enumerate(history_msgs[-5:], 1):
            role = m.get("role", "")
            content = m.get("content", "")
            role_label = "Ng∆∞·ªùi d√πng" if role == "user" else "Tr·ª£ l√Ω"
            lines.append(f"- {i}. {role_label}: {content}")
        history_block = "\nL·ªãch s·ª≠ h·ªôi tho·∫°i g·∫ßn ƒë√¢y:\n" + "\n".join(lines)

    # Danh s√°ch Top-K ƒëi·ªÅu lu·∫≠t ƒë·ªÉ m√¥ h√¨nh ch·ªçn
    context_lines = []
    for idx, d in enumerate(docs, 1):
        cited, chapter, title = law_line(d)
        content = (d.get("content") or "").strip()
        context_lines.append(f"{idx}) {cited}{chapter}{title}: {content}")
    context = "\n".join(context_lines) if context_lines else "‚ùå Kh√¥ng c√≥ ƒëi·ªÅu lu·∫≠t n√†o."

    prompt = dedent(f"""
    B·∫°n l√† **tr·ª£ l√Ω ph√°p lu·∫≠t** chuy√™n ph√¢n t√≠ch v√† t∆∞ v·∫•n theo **{law_name}**. 
    Vai tr√≤: gi·∫£i th√≠ch lu·∫≠t m·ªôt c√°ch ch√≠nh x√°c nh∆∞ng d·ªÖ hi·ªÉu, gi√∫p ng∆∞·ªùi d√¢n n·∫Øm r√µ quy ƒë·ªãnh v√† √°p d·ª•ng trong ƒë·ªùi s·ªëng. 
    Kh√¥ng ph·∫£i ch·ªâ ƒë·ªçc lu·∫≠t, m√† c·∫ßn l√†m r√µ √Ω nghƒ©a th·ª±c t·∫ø.

    Quy t·∫Øc b·∫Øt bu·ªôc:
    - Ch·ªâ tr·∫£ l·ªùi d·ª±a tr√™n c√°c ƒëi·ªÅu lu·∫≠t ƒë∆∞·ª£c cung c·∫•p b√™n d∆∞·ªõi.
    - N·∫øu kh√¥ng t√¨m th·∫•y quy ƒë·ªãnh ph√π h·ª£p ‚Üí tr·∫£ l·ªùi: "Kh√¥ng t√¨m th·∫•y quy ƒë·ªãnh trong vƒÉn b·∫£n ph√°p lu·∫≠t hi·ªán h√†nh."
    - Kh√¥ng b·ªãa, kh√¥ng ƒë∆∞a √Ω ki·∫øn c√° nh√¢n ngo√†i ph·∫°m vi lu·∫≠t.
    - ƒê·ªô d√†i c√¢u tr·∫£ l·ªùi ‚â§ 350 t·ª´.

    Y√™u c·∫ßu khi tr·∫£ l·ªùi:
    - Tr√≠ch d·∫´n ng·∫Øn g·ªçn ƒêi·ªÅu/Kho·∫£n/ƒêi·ªÉm v√† nguy√™n vƒÉn n·ªôi dung li√™n quan.
    - Gi·∫£i th√≠ch b·∫±ng ng√¥n ng·ªØ d·ªÖ hi·ªÉu, g·∫ßn g≈©i.
    - C√≥ th·ªÉ ƒë∆∞a v√≠ d·ª• th·ª±c t·∫ø minh h·ªça (n·∫øu ph√π h·ª£p).
    - Tr√¨nh b√†y theo c·∫•u tr√∫c:
      1) **T√≥m t·∫Øt c√¢u h·ªèi/t√¨nh hu·ªëng**
      2) **C∆° s·ªü ph√°p l√Ω** (tr√≠ch d·∫´n ng·∫Øn g·ªçn lu·∫≠t t·ª´ danh s√°ch b√™n d∆∞·ªõi)
      3) **Ph√¢n t√≠ch** ( c√≥ v√≠ d·ª• minh h·ªça nh∆∞ng v·∫´n ph·∫£i tr√≠ch d·∫´n c√°c ƒëi·ªÅu lu·∫≠t)
      4) **K·∫øt lu·∫≠n** (d·ª±a tr√™n ph√¢n t√≠ch ·ªü tr√™n, t√≥m t·∫Øt v·∫•n ƒë·ªÅ b·∫±ng ng√¥n ng·ªØ d·ªÖ hi·ªÉu, n√™u r√µ quy·ªÅn l·ª£i c·ªßa c√°c b√™n, h·∫≠u qu·∫£ ph√°p l√Ω v√† h∆∞·ªõng x·ª≠ l√Ω th·ª±c t·∫ø, tr√°nh tr√≠ch d·∫´n lu·∫≠t m√°y m√≥c)


    C√¢u h·ªèi hi·ªán t·∫°i:
    \"\"\"{query}\"\"\"{history_block}

    C√°c ƒëi·ªÅu lu·∫≠t Top-K (b·∫°n CH·ªà ƒë∆∞·ª£c vi·ªán d·∫´n trong danh s√°ch n√†y):
    {context}
    """).strip()

    return prompt


# ================== LLM STREAM ==================
def stream_answer(prompt, temperature=0.2):
    try:
        cfg = genai.types.GenerationConfig(
            temperature=float(temperature),
            max_output_tokens=512
        )
        resp = gemini_model.generate_content(
            prompt, generation_config=cfg, stream=True
        )

        for ch in resp:
            if getattr(ch, "text", None):
                yield ch.text  # xu·∫•t t·ª´ng ƒëo·∫°n nh·ªè ngay
    except Exception as e:
        yield f"\n\nL·ªói g·ªçi m√¥ h√¨nh: {e}"



# ================== STYLE ==================
CSS = """
:root {
  --brand: #1f2937; /* slate-800 */
  --accent: #4f46e5; /* indigo-600 */
}
.header {
  display:flex; align-items:center; gap:12px;
  padding: 8px 12px; border-radius: 14px;
  background: linear-gradient(135deg, #eef2ff, #f8fafc);
  border: 1px solid #e5e7eb;
}
.header .title {
  font-weight: 800; font-size: 20px; color: var(--brand);
}
.header .badge {
  font-size: 12px; padding: 4px 8px; border-radius: 999px;
  background:#eef2ff; color:#4338ca; border:1px solid #c7d2fe;
}
#chatbot { 
  height: 560px !important; 
  overflow-y: auto; /* Th√™m cu·ªôn d·ªçc */
  display: flex;
  flex-direction: column;
}
.chat-message {
  margin: 8px 0;
  padding: 8px 12px;
  border-radius: 8px;
  max-width: 80%;
}
.chat-message.user {
  background: var(--accent);
  color: white;
  align-self: flex-end;
}
.chat-message.assistant {
  background: #e5e7eb;
  color: var(--brand);
  align-self: flex-start;
}
.footer {
  font-size: 12px; opacity: .8; text-align:center; margin-top: 8px;
}
/* Khung cu·ªôn cho C∆° s·ªü ph√°p l√Ω */
#cites_md {
  max-height: 320px;
  overflow-y: auto;
}
"""

# ================== H√†m helper & core ==================
def format_history(history_msgs):
    """
    Chuy·ªÉn history_msgs sang format [{"role": "...", "content": "..."}, ...] cho gr.Chatbot
    """
    formatted = []
    for msg in history_msgs:
        role = msg.get("role")
        content = msg.get("content", "")
        # Gradio Chatbot ch·ªâ nh·∫≠n "user" ho·∫∑c "assistant"
        if role in ["user", "assistant"]:
            formatted.append({"role": role, "content": content})
    return formatted


def respond(message, history_msgs, k, temperature, cur_page_size, session_id):
    if not (message and message.strip()):
        gr.Info("Vui l√≤ng nh·∫≠p c√¢u h·ªèi.")
        return gr.update(), history_msgs, gr.update(), "", "", [], 1, "Trang 0/0", session_id

    # üîπ Generate session_id n·∫øu ch∆∞a c√≥
    if session_id is None:
        session_id = str(uuid.uuid4())

    # üîπ Ch·ªâ load history t·ª´ DB n·∫øu chatbot r·ªóng
    if not history_msgs:
        history_msgs = get_history_messages(session_id)

    # 0) Ph√¢n lo·∫°i c√¢u h·ªèi
    legal = is_legal_query(message)
    if not legal:
        reply = (
            "M√¨nh ch·ªß y·∫øu h·ªó tr·ª£ **c√°c v·∫•n ƒë·ªÅ ph√°p l√Ω theo Lu·∫≠t H√¥n nh√¢n & Gia ƒê√¨nh 2014**.\n\n"
            "B·∫°n c√≥ th·ªÉ cho m√¨nh bi·∫øt t√¨nh hu·ªëng ph√°p l√Ω c·ª• th·ªÉ (v√≠ d·ª•: *th·ªß t·ª•c ly h√¥n, quy·ªÅn nu√¥i con, chia t√†i s·∫£n, c·∫•p d∆∞·ª°ng...*)? "
            "N·∫øu c√¢u h·ªèi kh√¥ng thu·ªôc ph·∫°m vi n√†y, m√¨nh xin ph√©p kh√¥ng tra c·ª©u ƒë·ªÉ ti·∫øt ki·ªám t√†i nguy√™n."
        )
        mem = get_memory(session_id)
        mem.add_user_message(message)
        mem.add_ai_message(reply)

        updated_history = history_msgs + [
            {"role": "user", "content": message},
            {"role": "assistant", "content": reply},
        ]
        formatted_history = format_history(updated_history)

        return gr.update(value=""), formatted_history, gr.update(value="(Ch∆∞a c√≥ d·ªØ li·ªáu)"), reply, "", [], 1, "Trang 0/0", session_id

    # 1) T√¨m ƒëi·ªÅu lu·∫≠t
    try:
        docs = search_law(message, top_k=int(k))
    except Exception as e:
        err = f"L·ªói t√¨m ki·∫øm Qdrant: {e}"
        mem = get_memory(session_id)
        mem.add_user_message(message)
        mem.add_ai_message(err)

        updated_history = history_msgs + [
            {"role": "user", "content": message},
            {"role": "assistant", "content": err},
        ]
        formatted_history = format_history(updated_history)
        return gr.update(value=""), formatted_history, gr.update(value="(L·ªói tra c·ª©u)"), "", "(L·ªói tra c·ª©u)", [], 1, "Trang 0/0", session_id

    # 2) Render trang 1
    first_page = 1
    cites_markdown, page_label = docs_page_markdown(docs, first_page, int(cur_page_size))

    # 3) Prompt
    prompt = build_prompt(message, docs, history_msgs)

    # 4) Th√™m user v√†o memory
    mem = get_memory(session_id)
    mem.add_user_message(message)

    # 5) Stream assistant
    acc = ""
    for chunk in stream_answer(prompt, temperature=float(temperature)):
        acc += chunk
        # C·∫≠p nh·∫≠t hi·ªÉn th·ªã ki·ªÉu ChatGPT
        temp_history = history_msgs + [
            {"role": "user", "content": message},
            {"role": "assistant", "content": acc},
        ]
        formatted_history = format_history(temp_history)
        yield (
            gr.update(value=""),
            formatted_history,  # c·∫≠p nh·∫≠t to√†n b·ªô history
            gr.update(value=cites_markdown),
            acc,
            cites_markdown,
            docs,
            first_page,
            page_label,
            session_id  # Persist session_id
        )

    # üîπ Sau khi stream xong ‚Üí l∆∞u v√†o memory
    mem.add_ai_message(acc)

def on_clear(session_id):
    clear_history(session_id)
    return [], "(Ch∆∞a c√≥ d·ªØ li·ªáu)", "", "", [], 1, "Trang 0/0", None  # Reset session_id

# ================== UI ==================
with gr.Blocks(
    title="‚öñÔ∏è Tr·ª£ l√Ω Lu·∫≠t HN&Gƒê 2014",
    theme=gr.themes.Monochrome(primary_hue="indigo", neutral_hue="slate"),
    css=CSS
) as demo:
    # Header
    with gr.Row():
        gr.HTML("""
        <div class="header">
          <div style="font-size:24px">‚öñÔ∏è</div>
          <div class="title">Tr·ª£ l√Ω Lu·∫≠t H√¥n Nh√¢n & Gia ƒê√¨nh (2014)</div>
          <div class="badge">Lu·∫≠t s∆∞ ·∫£o tr·ª±c tuy·∫øn</div>
          <div class="badge">Ch·ªâ mang t√≠nh tham kh·∫£o</div>
        </div>
        """)

    with gr.Row():
        with gr.Column(scale=3):
            chatbot = gr.Chatbot(
                value=[],
                type="messages",
                show_copy_button=True,
                elem_id="chatbot",
                bubble_full_width=False,  # Tin nh·∫Øn kh√¥ng chi·∫øm to√†n chi·ªÅu r·ªông
            )
            gr.Markdown(
                "> üí° M·∫πo: M√¥ t·∫£ t√¨nh hu·ªëng (m·ªëc th·ªùi gian, t√†i s·∫£n, con chung, th·ªèa thu·∫≠n...) ƒë·ªÉ ph√¢n t√≠ch ch√≠nh x√°c h∆°n.",
                elem_classes=["card"]
            )
        with gr.Column(scale=2):
            with gr.Group():
                gr.Markdown("### ‚öôÔ∏è Tu·ª≥ ch·ªçn", elem_classes=["card"])
                with gr.Row():
                    topk = gr.Slider(5, 30, value=20, step=1, label="S·ªë ƒëi·ªÅu lu·∫≠t l·∫•y (Top-K)")
                    temp = gr.Slider(0.0, 1.0, value=0.2, step=0.05, label="Temperature (ƒê·ªô s√°ng t·∫°o c·ªßa m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn)")
            with gr.Group():
                gr.Markdown("### üßæ C∆° s·ªü ph√°p l√Ω (Top-K hi·ªÉn th·ªã ƒë·ªÉ ki·ªÉm tra)", elem_classes=["card"])
                cites_md = gr.Markdown(value="(Ch∆∞a c√≥ d·ªØ li·ªáu)", elem_id="cites_md")
                with gr.Row():
                    prev_page = gr.Button("‚¨ÖÔ∏è Trang tr∆∞·ªõc")
                    next_page = gr.Button("Trang sau ‚û°Ô∏è")
                with gr.Row():
                    page_info = gr.Markdown("Trang 0/0")
                    page_size = gr.Slider(3, 20, value=5, step=1, label="S·ªë m·ª•c m·ªói trang")

    with gr.Row():
        msg = gr.Textbox(
            placeholder="Nh·∫≠p c√¢u h·ªèi/t√¨nh hu·ªëng c·ªßa b·∫°n...",
            scale=4,
            autofocus=True,
            container=True,
        )
        send = gr.Button("G·ª≠i", variant="primary", scale=1)
        clear_btn = gr.Button("Xo√°", variant="secondary", scale=1)

    # States
    state_session = gr.State(None)  # S·ª≠a: None ƒë·ªÉ sinh UUID per session
    state_last_answer = gr.State("")
    state_last_cites = gr.State("")
    state_docs = gr.State([])
    state_page = gr.State(1)

    # -------- Click/Submit bindings --------
    send.click(
        respond,
        inputs=[msg, chatbot, topk, temp, page_size, state_session],
        outputs=[msg, chatbot, cites_md, state_last_answer, state_last_cites, state_docs, state_page, page_info, state_session],  # Th√™m state_session
        queue=True,
    )
    msg.submit(
        respond,
        inputs=[msg, chatbot, topk, temp, page_size, state_session],
        outputs=[msg, chatbot, cites_md, state_last_answer, state_last_cites, state_docs, state_page, page_info, state_session],  # Th√™m state_session
        queue=True,
    )

    clear_btn.click(
        on_clear,
        inputs=[state_session],
        outputs=[chatbot, cites_md, state_last_answer, state_last_cites, state_docs, state_page, page_info, state_session],  # Th√™m state_session
        queue=False
    )

    # -------- Like/Dislike --------
    def on_like(data: gr.LikeData):
        msg_like = data.value or {}
        role = msg_like.get("role", "assistant")
        text = msg_like.get("content", "")
        print(f"[VOTE] liked={data.liked} | role={role} | text={(text[:120]+'...') if len(text)>120 else text}")
        return None

    chatbot.like(on_like)

    # -------- Pagination Handlers --------
    def render_cites_for_page(docs, page, cur_page_size):
        md, label = docs_page_markdown(docs or [], int(page), int(cur_page_size))
        return gr.update(value=md), int(page), label

    def go_prev(docs, page, cur_page_size):
        if not docs:
            return render_cites_for_page([], 1, cur_page_size)
        _, _, _, _ = paginate_docs(docs, 1, int(cur_page_size))
        new_page = max(1, int(page) - 1)
        return render_cites_for_page(docs, new_page, cur_page_size)

    def go_next(docs, page, cur_page_size):
        if not docs:
            return render_cites_for_page([], 1, cur_page_size)
        _, total, total_pages, _ = paginate_docs(docs, 1, int(cur_page_size))
        new_page = min(total_pages if total_pages > 0 else 1, int(page) + 1)
        return render_cites_for_page(docs, new_page, cur_page_size)

    def on_change_page_size(docs, cur_page_size):
        return render_cites_for_page(docs, 1, cur_page_size)

    prev_page.click(
        go_prev,
        inputs=[state_docs, state_page, page_size],
        outputs=[cites_md, state_page, page_info],
        queue=False,
    )
    next_page.click(
        go_next,
        inputs=[state_docs, state_page, page_size],
        outputs=[cites_md, state_page, page_info],
        queue=False,
    )
    page_size.release(
        on_change_page_size,
        inputs=[state_docs, page_size],
        outputs=[cites_md, state_page, page_info],
        queue=False,
    )

    # Footer
    gr.HTML(f"""
    <div class="footer">
      ¬© {datetime.now().year} ‚Äî Tr·ª£ l√Ω t∆∞ v·∫•n d·ª±a tr√™n Lu·∫≠t H√¥n Nh√¢n & Gia ƒê√¨nh 2014.
      N·ªôi dung ch·ªâ mang t√≠nh tham kh·∫£o, kh√¥ng thay th·∫ø t∆∞ v·∫•n ph√°p l√Ω ch√≠nh th·ª©c.
    </div>
    """)


if __name__ == "__main__":
    demo.launch()