# botchat_honnhan.py
import os
from datetime import datetime
from textwrap import dedent

import gradio as gr
from dotenv import load_dotenv
from qdrant_client import QdrantClient
from sentence_transformers import SentenceTransformer
import google.generativeai as genai

# ================== ENV ==================
load_dotenv()
QDRANT_URL = os.getenv("QDRANT_URL", "").strip()
QDRANT_API_KEY = os.getenv("QDRANT_API_KEY", "").strip()
COLLECTION_NAME = os.getenv("COLLECTION_NAME", "luat_hon_nhan_va_gia_dinh_2014")
EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL", "BAAI/bge-m3")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY", "").strip()
GEMINI_MODEL_ID = os.getenv("GEMINI_MODEL_ID", "gemini-2.5-flash")

if not (QDRANT_URL and QDRANT_API_KEY and GEMINI_API_KEY):
    raise RuntimeError("Thi·∫øu QDRANT_URL / QDRANT_API_KEY / GEMINI_API_KEY trong .env")

# ================== INIT ==================
client = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY, prefer_grpc=True)
embedder = SentenceTransformer(EMBEDDING_MODEL)

genai.configure(api_key=GEMINI_API_KEY)
gemini_model = genai.GenerativeModel(GEMINI_MODEL_ID)

# ================== SEARCH HELPERS ==================
def search_law(query: str, top_k: int = 15):
    """
    T√¨m ki·∫øm trong Qdrant v√† tr·∫£ v·ªÅ danh s√°ch ƒëi·ªÅu lu·∫≠t + ƒëi·ªÉm t∆∞∆°ng ƒë·ªìng.
    V·ªõi BAAI/bge-m3 n√™n normalize ƒë·ªÉ k·∫øt qu·∫£ ·ªïn ƒë·ªãnh.
    """
    vec = embedder.encode([query], normalize_embeddings=True)[0].tolist()
    results = client.search(
        collection_name=COLLECTION_NAME,
        query_vector=vec,
        limit=max(1, min(int(top_k), 50)),
        with_payload=True
    )

    docs = []
    for r in results:
        p = r.payload or {}
        docs.append({
            "citation": p.get("exact_citation", ""),     # v√≠ d·ª•: "ƒêi·ªÅu 56, Kho·∫£n 1, Lu·∫≠t HN&Gƒê 2014"
            "chapter": p.get("chapter", ""),
            "article_no": p.get("article_no", ""),
            "article_title": p.get("article_title", ""),
            "clause_no": p.get("clause_no", ""),
            "point_letter": p.get("point_letter", ""),
            "content": (p.get("content") or "").strip(),
            "score": float(r.score or 0.0),
        })
    return docs

def law_line(d):
    cited = d.get("citation") or (
        f"ƒêi·ªÅu {d.get('article_no','')}"
        + (f", Kho·∫£n {d.get('clause_no')}" if d.get('clause_no') else "")
        + (f", ƒêi·ªÉm {d.get('point_letter')}" if d.get('point_letter') else "")
    )
    chapter = f" ({d.get('chapter')})" if d.get("chapter") else ""
    title = f" ‚Äî {d.get('article_title')}" if d.get("article_title") else ""
    return cited, chapter, title

def docs_to_markdown(docs):
    """
    Hi·ªÉn th·ªã Top-K ƒëi·ªÅu lu·∫≠t ·ªü d·∫°ng Markdown (tr√°nh l·ªói [Object Object]).
    """
    if not docs:
        return "‚ùå Kh√¥ng t√¨m th·∫•y ƒëi·ªÅu lu·∫≠t n√†o."
    lines = []
    for i, d in enumerate(docs, 1):
        cited, chapter, title = law_line(d)
        content = (d.get("content") or "").strip()
        score = round(d.get("score", 0.0), 4)
        lines.append(
            f"**{i}. {cited}{chapter}{title}**  \n"
            f"{content}  \n"
            f"<sub>ƒê·ªô li√™n quan: {score}</sub>\n"
        )
    return "\n".join(lines)

# -------- Ph√¢n trang cho c∆° s·ªü ph√°p l√Ω --------
def paginate_docs(docs, page: int, page_size: int):
    total = len(docs)
    if total == 0:
        return [], 0, 0, 0
    page = max(1, int(page))
    page_size = max(1, int(page_size))
    start = (page - 1) * page_size
    end = start + page_size
    sliced = docs[start:end]
    total_pages = (total + page_size - 1) // page_size
    return sliced, total, total_pages, start

def docs_page_markdown(docs, page: int, page_size: int):
    sliced, total, total_pages, start = paginate_docs(docs, page, page_size)
    if total == 0:
        return "(Ch∆∞a c√≥ d·ªØ li·ªáu)", "Trang 0/0"
    body = docs_to_markdown(sliced)
    page_label = f"Trang {page}/{total_pages} ‚Äî hi·ªÉn th·ªã {start+1}‚Äì{min(start+len(sliced), total)} / {total}"
    return f"**{page_label}**\n\n{body}", page_label

# ================== CLASSIFY: c√≥ li√™n quan ph√°p l√Ω? ==================
def is_legal_query(user_query: str) -> bool:
    """
    D√πng Gemini ƒë·ªÉ ph√¢n lo·∫°i nhanh xem c√¢u h·ªèi c√≥ *li√™n quan t·ªõi ph√°p l√Ω h√¥n nh√¢n & gia ƒë√¨nh VN 2014* hay kh√¥ng.
    Tr·∫£ v·ªÅ True n·∫øu LI√äN QUAN, False n·∫øu kh√¥ng.
    """
    prompt = dedent(f"""
    H√£y ph√¢n lo·∫°i c√¢u sau c√≥ LI√äN QUAN ƒë·∫øn t∆∞ v·∫•n ph√°p l√Ω theo Lu·∫≠t H√¥n nh√¢n & Gia ƒë√¨nh Vi·ªát Nam 2014 hay kh√¥ng.

    Y√äU C·∫¶U:
    - N·∫øu LI√äN QUAN: tr·∫£ v·ªÅ ƒë√∫ng m·ªôt t·ª´ "LEGAL".
    - N·∫øu KH√îNG LI√äN QUAN (x√£ giao, chitchat, th·ªùi ti·∫øt, c√¥ng ngh·ªá, c√°c lu·∫≠t kh√°c...): tr·∫£ v·ªÅ ƒë√∫ng m·ªôt t·ª´ "NONLEGAL".
    - Kh√¥ng th√™m l·ªùi gi·∫£i th√≠ch.

    C√ÇU C·∫¶N PH√ÇN LO·∫†I:
    ---
    {user_query}
    ---
    """).strip()

    try:
        cfg = genai.types.GenerationConfig(temperature=0.0)
        resp = gemini_model.generate_content(prompt, generation_config=cfg)
        text = (getattr(resp, "text", None) or "").strip().upper()
        if "LEGAL" in text and "NON" not in text:
            return True
        if text == "NONLEGAL" or "NONLEGAL" in text:
            return False
        # N·∫øu model tr·∫£ r√°c, fallback heuristic ƒë∆°n gi·∫£n theo t·ª´ kh√≥a ph√°p l√Ω
        keywords = [
            "ly h√¥n", "ly hon", "k·∫øt h√¥n", "ket hon", "h√¥n nh√¢n", "hon nhan",
            "con chung", "nu√¥i con", "cap duong", "c·∫•p d∆∞·ª°ng", "t√†i s·∫£n chung",
            "chia t√†i s·∫£n", "gi√†nh quy·ªÅn", "gi√°m h·ªô", "giam ho",
            "ly th√¢n", "ly than", "ƒëi·ªÅu", "kho·∫£n", "ƒëi·ªÉm", "to√†", "t√≤a", "to√† √°n", "t√≤a √°n",
            "gi·∫•y ƒëƒÉng k√Ω k·∫øt h√¥n", "h·ªßy k·∫øt h√¥n", "c·∫•m k·∫øt h√¥n"
        ]
        q = user_query.lower()
        return any(k in q for k in keywords)
    except Exception:
        # L·ªói g·ªçi model => d√πng heuristic
        keywords = [
            "ly h√¥n", "ly hon", "k·∫øt h√¥n", "ket hon", "h√¥n nh√¢n", "hon nhan",
            "con chung", "nu√¥i con", "cap duong", "c·∫•p d∆∞·ª°ng", "t√†i s·∫£n chung",
            "chia t√†i s·∫£n", "gi√†nh quy·ªÅn", "gi√°m h·ªô", "giam ho",
            "ly th√¢n", "ly than", "ƒëi·ªÅu", "kho·∫£n", "ƒëi·ªÉm", "to√†", "t√≤a", "to√† √°n", "t√≤a √°n",
            "gi·∫•y ƒëƒÉng k√Ω k·∫øt h√¥n", "h·ªßy k·∫øt h√¥n", "c·∫•m k·∫øt h√¥n"
        ]
        q = user_query.lower()
        return any(k in q for k in keywords)

# ================== PROMPT ==================
def build_prompt(query: str, docs, history_msgs):
    # L·ªãch s·ª≠ g·ªçn 6 l∆∞·ª£t g·∫ßn nh·∫•t
    history_block = ""
    if history_msgs:
        lines = []
        for i, m in enumerate(history_msgs[-6:], 1):
            role = m.get("role", "")
            content = m.get("content", "")
            lines.append(f"- {i}. {role}: {content}")
        history_block = "\nL·ªãch s·ª≠ h·ªôi tho·∫°i g·∫ßn ƒë√¢y:\n" + "\n".join(lines)

    # Danh s√°ch Top-K ƒëi·ªÅu lu·∫≠t cho m√¥ h√¨nh t·ª± ch·ªçn v√† vi·ªán d·∫´n
    context_lines = []
    for idx, d in enumerate(docs, 1):
        cited, chapter, title = law_line(d)
        content = (d.get("content") or "").strip()
        context_lines.append(f"{idx}) {cited}{chapter}{title}: {content}")
    context = "\n".join(context_lines) if context_lines else "‚ùå Kh√¥ng c√≥ ƒëi·ªÅu lu·∫≠t n√†o."

    prompt = dedent(f"""
    B·∫°n l√† **tr·ª£ l√Ω ph√°p l√Ω** chuy√™n v·ªÅ **Lu·∫≠t H√¥n nh√¢n v√† Gia ƒë√¨nh Vi·ªát Nam**, tr·∫£ l·ªùi v·ªõi **phong th√°i c·ªßa m·ªôt lu·∫≠t s∆∞**.
    Ch·ª©c nƒÉng ch√≠nh c·ªßa b·∫°n:
    - Gi·∫£i ƒë√°p th·∫Øc m·∫Øc v·ªÅ ph√°p lu·∫≠t h√¥n nh√¢n v√† gia ƒë√¨nh.
    - T√¨m ki·∫øm c√°c ƒëi·ªÅu lu·∫≠t ƒë∆∞·ª£c y√™u c·∫ßu (ƒë√£ cung c·∫•p b√™n d∆∞·ªõi) v√† tr·∫£ l·ªùi ng∆∞·ªùi d√πng.
    - Khi c√≥ danh s√°ch Top-K ƒëi·ªÅu lu·∫≠t, **t·ª± ch·ªçn c√°c ƒëi·ªÅu ph√π h·ª£p nh·∫•t** ƒë·ªÉ tr·∫£ l·ªùi v√† **ph·∫£i tr√≠ch d·∫´n chi ti·∫øt** (ƒêi·ªÅu/Kho·∫£n/ƒêi·ªÉm, nguy√™n vƒÉn n·ªôi dung), k√®m **gi·∫£i th√≠ch r√µ r√†ng**.
    - N·∫øu **kh√¥ng c√≥ ƒëi·ªÅu lu·∫≠t ph√π h·ª£p** trong danh s√°ch, n√≥i r√µ **kh√¥ng c√≥**; **tuy·ªát ƒë·ªëi kh√¥ng ƒë∆∞·ª£c b·ªãa**.

    Y√™u c·∫ßu tr·∫£ l·ªùi ƒë·∫ßy ƒë·ªß, chi ti·∫øt, d·ªÖ hi·ªÉu, d·ªÖ √°p d·ª•ng ƒë·ªëi v·ªõi nh·ªØng c√¢u h·ªèi ph·ª©c t·∫°p, c√≤n nh·ªØng c√¢u h·ªèi ƒë∆°n gi·∫£n th√¨ c√≥ th·ªÉ tr·∫£ l·ªùi ng·∫Øn g·ªçn nh∆∞ng v·∫´n ƒë·∫ßy ƒë·ªß √Ω:
    - Lu√¥n ƒë·∫∑t c√¢u tr·∫£ l·ªùi trong b·ªëi c·∫£nh ph√°p lu·∫≠t Vi·ªát Nam hi·ªán h√†nh.
    - VƒÉn phong chu·∫©n m·ª±c, m·∫°ch l·∫°c, l·∫≠p lu·∫≠n theo logic ph√°p l√Ω.
    - C·∫•u tr√∫c ƒë·ªÅ xu·∫•t:
      1) **T√≥m t·∫Øt c√¢u h·ªèi/t√¨nh hu·ªëng** (n·∫øu ph√π h·ª£p)
      2) **C∆° s·ªü ph√°p l√Ω ƒë∆∞·ª£c tr√≠ch d·∫´n** (ch·ªâ t·ª´ c√°c ƒëi·ªÅu lu·∫≠t d∆∞·ªõi ƒë√¢y; ghi r√µ ƒêi·ªÅu/Kho·∫£n/ƒêi·ªÉm; tr√≠ch NGUY√äN VƒÇN)
      3) **Ph√¢n t√≠ch** (gi·∫£i th√≠ch v√† √°p d·ª•ng v√†o t√¨nh hu·ªëng; n√™u ƒëi·ªÅu ki·ªán √°p d·ª•ng/ngo·∫°i l·ªá n·∫øu c√≥)
      4) **K·∫øt lu·∫≠n/H∆∞·ªõng x·ª≠ l√Ω**
      5) **L∆∞u √Ω**: "Th√¥ng tin ch·ªâ mang t√≠nh tham kh·∫£o, kh√¥ng thay th·∫ø t∆∞ v·∫•n ph√°p l√Ω ch√≠nh th·ª©c."
    - N·∫øu c√¢u h·ªèi **kh√¥ng thu·ªôc ph·∫°m vi** Lu·∫≠t HN&Gƒê 2014: l·ªãch s·ª± t·ª´ ch·ªëi v√† n√™u ph·∫°m vi b·∫°n h·ªó tr·ª£.

    C√¢u h·ªèi hi·ªán t·∫°i c·ªßa ng∆∞·ªùi d√πng:
    \"\"\"{query}\"\"\"{history_block}

    C√°c ƒëi·ªÅu lu·∫≠t Top-K (ƒë·ªÉ b·∫°n l·ª±a ch·ªçn khi l·∫≠p lu·∫≠n, KH√îNG ƒë∆∞·ª£c vi·ªán d·∫´n ngo√†i danh s√°ch n√†y):
    {context}
    """).strip()

    return prompt

# ================== LLM STREAM ==================
def stream_answer(prompt, temperature=0.2):
    try:
        cfg = genai.types.GenerationConfig(
            temperature=float(temperature),
        )
        resp = gemini_model.generate_content(prompt, generation_config=cfg, stream=True)
        for ch in resp:
            if getattr(ch, "text", None):
                yield ch.text
    except Exception as e:
        yield f"\n\nL·ªói g·ªçi m√¥ h√¨nh: {e}"

# ================== STYLE ==================
CSS = """
:root {
  --brand: #1f2937; /* slate-800 */
  --accent: #4f46e5; /* indigo-600 */
}
.header {
  display:flex; align-items:center; gap:12px;
  padding: 8px 12px; border-radius: 14px;
  background: linear-gradient(135deg, #eef2ff, #f8fafc);
  border: 1px solid #e5e7eb;
}
.header .title {
  font-weight: 800; font-size: 20px; color: var(--brand);
}
.header .badge {
  font-size: 12px; padding: 4px 8px; border-radius: 999px;
  background:#eef2ff; color:#4338ca; border:1px solid #c7d2fe;
}
#chatbot { height: 560px !important; }
.card {
  border: 1px solid #e5e7eb; border-radius: 16px; padding: 12px; background: #ffffffaa;
  backdrop-filter: blur(8px);
}
.footer {
  font-size: 12px; opacity: .8; text-align:center; margin-top: 8px;
}
/* Khung cu·ªôn cho C∆° s·ªü ph√°p l√Ω */
#cites_md {
  max-height: 320px;
  overflow-y: auto;
}
"""

# ================== UI ==================
with gr.Blocks(
    title="‚öñÔ∏è Tr·ª£ l√Ω Lu·∫≠t HN&Gƒê 2014",
    theme=gr.themes.Monochrome(primary_hue="indigo", neutral_hue="slate"),
    css=CSS
) as demo:
    # Header
    with gr.Row():
        gr.HTML("""
        <div class="header">
          <div style="font-size:24px">‚öñÔ∏è</div>
          <div class="title">Tr·ª£ l√Ω Lu·∫≠t H√¥n Nh√¢n & Gia ƒê√¨nh (2014)</div>
          <div class="badge">Lu·∫≠t s∆∞ ·∫£o tr·ª±c tuy·∫øn</div>
          <div class="badge">Ch·ªâ mang t√≠nh tham kh·∫£o</div>
        </div>
        """)

    with gr.Row():
        with gr.Column(scale=3):
            chatbot = gr.Chatbot(
                value=[],
                type="messages",          # schema messages c·ªßa Gradio 5
                bubble_full_width=False,
                show_copy_button=True,
                elem_id="chatbot",
            )
            gr.Markdown(
                "> üí° M·∫πo: M√¥ t·∫£ t√¨nh hu·ªëng (m·ªëc th·ªùi gian, t√†i s·∫£n, con chung, th·ªèa thu·∫≠n...) ƒë·ªÉ ph√¢n t√≠ch ch√≠nh x√°c h∆°n.",
                elem_classes=["card"]
            )
        with gr.Column(scale=2):
            with gr.Group():
                gr.Markdown("### ‚öôÔ∏è Tu·ª≥ ch·ªçn", elem_classes=["card"])
                with gr.Row():
                    topk = gr.Slider(5, 30, value=15, step=1, label="S·ªë ƒëi·ªÅu lu·∫≠t l·∫•y (Top-K)")
                    temp = gr.Slider(0.0, 1.0, value=0.2, step=0.05, label="Temperature (ƒê·ªô s√°ng t·∫°o c·ªßa m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn)")
            with gr.Group():
                gr.Markdown("### üßæ C∆° s·ªü ph√°p l√Ω (Top-K hi·ªÉn th·ªã ƒë·ªÉ ki·ªÉm tra)", elem_classes=["card"])
                # Khung Markdown c√≥ chi·ªÅu cao c·ªë ƒë·ªãnh v√† scroll
                cites_md = gr.Markdown(value="(Ch∆∞a c√≥ d·ªØ li·ªáu)", elem_id="cites_md")
                with gr.Row():
                    prev_page = gr.Button("‚¨ÖÔ∏è Trang tr∆∞·ªõc")
                    next_page = gr.Button("Trang sau ‚û°Ô∏è")
                with gr.Row():
                    page_info = gr.Markdown("Trang 0/0")
                    page_size = gr.Slider(3, 20, value=5, step=1, label="S·ªë m·ª•c m·ªói trang")

    with gr.Row():
        msg = gr.Textbox(
            placeholder="Nh·∫≠p c√¢u h·ªèi/t√¨nh hu·ªëng c·ªßa b·∫°n...",
            scale=4,
            autofocus=True,
            container=True,
        )
        send = gr.Button("G·ª≠i", variant="primary", scale=1)
        clear = gr.Button("Xo√°", variant="secondary", scale=1)

    # States
    state_history = gr.State([])      # l·ªãch s·ª≠ chat theo schema messages
    state_last_answer = gr.State("")  # v·∫´n gi·ªØ ƒë·ªÉ ti·ªán debug/n·ªôi b·ªô n·∫øu c·∫ßn
    state_last_cites = gr.State("")   # markdown ƒë√£ render
    state_docs = gr.State([])         # l∆∞u full docs c·ªßa l·∫ßn tra c·ª©u hi·ªán t·∫°i
    state_page = gr.State(1)          # trang hi·ªán t·∫°i

    # -------- Core Handler (Streaming) --------
    def respond(message, history_msgs, k, temperature, cur_page_size):
        if not (message and message.strip()):
            gr.Info("Vui l√≤ng nh·∫≠p c√¢u h·ªèi.")
            return gr.update(), history_msgs, gr.update(), "", "", [], 1, "Trang 0/0"

        # 0) Ph√¢n lo·∫°i: c√≥ li√™n quan ph√°p l√Ω HN&Gƒê 2014?
        legal = is_legal_query(message)
        if not legal:
            # Kh√¥ng tra c·ª©u Qdrant; tr·∫£ l·ªùi ng·∫Øn g·ªçn + g·ª£i √Ω
            reply = (
                "M√¨nh ch·ªß y·∫øu h·ªó tr·ª£ **c√°c v·∫•n ƒë·ªÅ ph√°p l√Ω theo Lu·∫≠t H√¥n nh√¢n & Gia ƒë√¨nh 2014**.\n\n"
                "B·∫°n c√≥ th·ªÉ cho m√¨nh bi·∫øt t√¨nh hu·ªëng ph√°p l√Ω c·ª• th·ªÉ (v√≠ d·ª•: *th·ªß t·ª•c ly h√¥n, quy·ªÅn nu√¥i con, chia t√†i s·∫£n, c·∫•p d∆∞·ª°ng...*)? "
                "N·∫øu c√¢u h·ªèi kh√¥ng thu·ªôc ph·∫°m vi n√†y, m√¨nh xin ph√©p kh√¥ng tra c·ª©u ƒë·ªÉ ti·∫øt ki·ªám t√†i nguy√™n."
            )
            upd = history_msgs + [
                {"role": "user", "content": message},
                {"role": "assistant", "content": reply},
            ]
            # Reset khu c∆° s·ªü ph√°p l√Ω
            return gr.update(value=""), upd, gr.update(value="(Ch∆∞a c√≥ d·ªØ li·ªáu)"), reply, "", [], 1, "Trang 0/0"

        # 1) T√¨m ƒëi·ªÅu lu·∫≠t (ch·ªâ ch·∫°y khi legal=True)
        try:
            docs = search_law(message, top_k=int(k))
        except Exception as e:
            err = f"L·ªói t√¨m ki·∫øm Qdrant: {e}"
            upd = history_msgs + [
                {"role":"user","content":message},
                {"role":"assistant","content":err},
            ]
            return gr.update(value=""), upd, gr.update(value="(L·ªói tra c·ª©u)"), "", "(L·ªói tra c·ª©u)", [], 1, "Trang 0/0"

        # 2) Render trang 1 cho C∆° s·ªü ph√°p l√Ω
        first_page = 1
        cites_markdown, page_label = docs_page_markdown(docs, first_page, int(cur_page_size))

        # 3) T·∫°o prompt theo y√™u c·∫ßu
        prompt = build_prompt(message, docs, history_msgs)

        # 4) ƒê·∫©y user + placeholder assistant (ƒê√öNG SCHEMA V5)
        history_msgs = history_msgs + [
            {"role": "user", "content": message},
            {"role": "assistant", "content": ""},   # stream ƒë·ªï v√†o ƒë√¢y
        ]

        # 5) Stream k·∫øt qu·∫£ v√†o message cu·ªëi
        acc = ""
        for chunk in stream_answer(prompt, temperature=float(temperature)):
            acc += chunk
            history_msgs[-1]["content"] = acc
            yield (
                gr.update(value=""),                 # clear √¥ nh·∫≠p
                history_msgs,                        # c·∫≠p nh·∫≠t Chatbot
                gr.update(value=cites_markdown),     # Markdown c∆° s·ªü ph√°p l√Ω (trang 1)
                acc,                                 # l∆∞u ƒë·ªÉ debug/n·ªôi b·ªô
                cites_markdown,                      # l∆∞u markdown hi·ªÉn th·ªã
                docs,                                # state_docs
                first_page,                          # state_page
                page_label                           # page_info
            )

    send.click(
        respond,
        inputs=[msg, state_history, topk, temp, page_size],
        outputs=[msg, chatbot, cites_md, state_last_answer, state_last_cites, state_docs, state_page, page_info],
        queue=True,
    )
    msg.submit(
        respond,
        inputs=[msg, state_history, topk, temp, page_size],
        outputs=[msg, chatbot, cites_md, state_last_answer, state_last_cites, state_docs, state_page, page_info],
        queue=True,
    )

    # -------- Like/Dislike (Gradio 5) --------
    def on_like(data: gr.LikeData):
        msg_like = data.value or {}
        role = msg_like.get("role", "assistant")
        text = msg_like.get("content", "")
        print(f"[VOTE] liked={data.liked} | role={role} | text={(text[:120]+'...') if len(text)>120 else text}")
        return None

    chatbot.like(on_like)

    # -------- Pagination Handlers --------
    def render_cites_for_page(docs, page, cur_page_size):
        md, label = docs_page_markdown(docs or [], int(page), int(cur_page_size))
        return gr.update(value=md), int(page), label

    def go_prev(docs, page, cur_page_size):
        if not docs:
            return render_cites_for_page([], 1, cur_page_size)
        _, _, _, _ = paginate_docs(docs, 1, int(cur_page_size))
        new_page = max(1, int(page) - 1)
        return render_cites_for_page(docs, new_page, cur_page_size)

    def go_next(docs, page, cur_page_size):
        if not docs:
            return render_cites_for_page([], 1, cur_page_size)
        _, total, total_pages, _ = paginate_docs(docs, 1, int(cur_page_size))
        new_page = min(total_pages if total_pages > 0 else 1, int(page) + 1)
        return render_cites_for_page(docs, new_page, cur_page_size)

    def on_change_page_size(docs, cur_page_size):
        # Khi ƒë·ªïi page_size, quay v·ªÅ trang 1
        return render_cites_for_page(docs, 1, cur_page_size)

    prev_page.click(
        go_prev,
        inputs=[state_docs, state_page, page_size],
        outputs=[cites_md, state_page, page_info],
        queue=False,
    )
    next_page.click(
        go_next,
        inputs=[state_docs, state_page, page_size],
        outputs=[cites_md, state_page, page_info],
        queue=False,
    )
    page_size.release(
        on_change_page_size,
        inputs=[state_docs, page_size],
        outputs=[cites_md, state_page, page_info],
        queue=False,
    )

    # -------- Clear --------
    def on_clear():
        return [], "(Ch∆∞a c√≥ d·ªØ li·ªáu)", "", "", [], 1, "Trang 0/0"

    clear.click(
        on_clear,
        None,
        [chatbot, cites_md, state_last_answer, state_last_cites, state_docs, state_page, page_info],
        queue=False
    )

    # Footer
    gr.HTML(f"""
    <div class="footer">
      ¬© {datetime.now().year} ‚Äî Tr·ª£ l√Ω t∆∞ v·∫•n d·ª±a tr√™n Lu·∫≠t H√¥n Nh√¢n & Gia ƒê√¨nh 2014.
      N·ªôi dung ch·ªâ mang t√≠nh tham kh·∫£o, kh√¥ng thay th·∫ø t∆∞ v·∫•n ph√°p l√Ω ch√≠nh th·ª©c.
    </div>
    """)

if __name__ == "__main__":
    demo.launch(show_error=True)
